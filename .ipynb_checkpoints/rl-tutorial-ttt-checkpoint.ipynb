{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning series: Tic-tac-toe game bot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-24 15:29:08.828119: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-24 15:29:08.937592: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-03-24 15:29:09.451088: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-24 15:29:09.451139: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-24 15:29:09.451144: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIR = tmp/rl-tutorial-ttt\n"
     ]
    }
   ],
   "source": [
    "### IMPORTS ###\n",
    "\n",
    "ipynb = 'rl-tutorial-ttt'\n",
    "\n",
    "import sys, os, json\n",
    "import tensorflow    as tf\n",
    "import keras.backend as K\n",
    "\n",
    "from ipywidgets           import widgets, HBox, VBox, Layout\n",
    "from IPython.display      import display, HTML, Javascript as JS\n",
    "from pandas               import DataFrame\n",
    "from pathlib              import Path\n",
    "from pprint               import pprint\n",
    "from operator             import iconcat\n",
    "from functools            import reduce, partial\n",
    "from collections          import deque\n",
    "from numpy                import *\n",
    "from numpy.random         import *\n",
    "from os.path              import isfile\n",
    "from uuid                 import uuid4 as guid\n",
    "\n",
    "from keras.layers         import Input, Dense, BatchNormalization, Activation, Multiply\n",
    "from keras.losses         import mse, categorical_crossentropy, binary_crossentropy\n",
    "from keras.optimizers     import Adam\n",
    "from keras.regularizers   import l2\n",
    "from keras.activations    import softmax\n",
    "from keras.models         import Model, load_model, clone_model\n",
    "from keras.callbacks      import LearningRateScheduler, LambdaCallback\n",
    "from keras.utils          import Progbar, to_categorical\n",
    "\n",
    "from matplotlib.pyplot    import *\n",
    "from time                 import *\n",
    "\n",
    "# %matplotlib inline\n",
    "\n",
    "DIR = f'tmp/{ipynb}'\n",
    "Path(DIR).mkdir(parents=True, exist_ok=True)\n",
    "print('DIR =', DIR)\n",
    "\n",
    "TESTS = 100\n",
    "\n",
    "def rename(newname):\n",
    "    def decorator(f):\n",
    "        f.__name__ = newname\n",
    "        return f\n",
    "    return decorator\n",
    "\n",
    "def time2str(t):\n",
    "    return strftime(\"%b %d %Y %H:%M:%S\", localtime(t))\n",
    "\n",
    "def mtime(path):\n",
    "    return os.path.getmtime(path)\n",
    "\n",
    "def mtime2str(path):\n",
    "    return time2str(mtime(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem formulation: Treating after-states as action-value function inputs\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 576us/step - samples per episode: 6.9000 - total samples: 69.0000\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 2.]\n",
      " [0. 0. 0. 1. 0. 0. 1. 0. 2.]\n",
      " [0. 0. 0. 1. 2. 0. 1. 0. 2.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [2. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [2. 1. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [2. 1. 0. 0. 0. 2. 0. 0. 1.]\n",
      " [2. 1. 0. 0. 0. 2. 1. 0. 1.]\n",
      " [2. 1. 2. 0. 0. 2. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 2. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 2. 1. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 2. 1. 2. 0. 0. 0. 1. 0.]\n",
      " [0. 2. 1. 2. 1. 0. 0. 1. 0.]\n",
      " [0. 2. 1. 2. 1. 0. 2. 1. 0.]\n",
      " [0. 2. 1. 2. 1. 1. 2. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 2. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 2. 0. 1. 0. 1. 0. 0. 0.]\n",
      " [0. 2. 0. 1. 2. 1. 0. 0. 0.]\n",
      " [0. 2. 0. 1. 2. 1. 1. 0. 0.]\n",
      " [0. 2. 0. 1. 2. 1. 1. 0. 2.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 2. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 1. 2. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 1. 2. 2. 0. 0.]\n",
      " [1. 1. 0. 0. 1. 2. 2. 0. 0.]\n",
      " [1. 1. 2. 0. 1. 2. 2. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 2. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 2. 0. 1.]\n",
      " [2. 0. 1. 0. 0. 0. 2. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [2. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [2. 0. 0. 1. 0. 1. 0. 0. 0.]\n",
      " [2. 2. 0. 1. 0. 1. 0. 0. 0.]\n",
      " [2. 2. 0. 1. 0. 1. 0. 0. 1.]\n",
      " [2. 2. 0. 1. 2. 1. 0. 0. 1.]\n",
      " [2. 2. 0. 1. 2. 1. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 2. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 1. 2. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 1. 2. 0. 0. 2.]\n",
      " [0. 1. 0. 0. 1. 2. 1. 0. 2.]\n",
      " [0. 1. 0. 0. 1. 2. 1. 2. 2.]\n",
      " [0. 1. 0. 1. 1. 2. 1. 2. 2.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [2. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [2. 0. 1. 0. 0. 0. 0. 0. 1.]\n",
      " [2. 2. 1. 0. 0. 0. 0. 0. 1.]\n",
      " [2. 2. 1. 0. 1. 0. 0. 0. 1.]\n",
      " [2. 2. 1. 0. 1. 2. 0. 0. 1.]\n",
      " [2. 2. 1. 1. 1. 2. 0. 0. 1.]\n",
      " [2. 2. 1. 1. 1. 2. 0. 2. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 2. 0. 0. 0.]\n",
      " [1. 0. 1. 0. 0. 2. 0. 0. 0.]\n",
      " [1. 0. 1. 0. 0. 2. 0. 2. 0.]]\n",
      "[3. 8. 6. 4. 0. 1. 0. 8. 5. 6. 2. 7. 7. 1. 2. 3. 4. 6. 5. 0. 5. 1. 3. 4.\n",
      " 6. 8. 0. 4. 5. 0. 6. 1. 2. 8. 8. 6. 2. 0. 5. 3. 0. 5. 1. 8. 4. 6. 2. 4.\n",
      " 5. 1. 8. 6. 7. 3. 2. 8. 0. 2. 1. 4. 5. 3. 7. 6. 2. 5. 0. 7. 1.]\n",
      "[0.11111111 0.125      0.14285714 0.16666667 0.2        0.11111111\n",
      " 0.125      0.14285714 0.16666667 0.2        0.25       0.33333333\n",
      " 0.11111111 0.125      0.14285714 0.16666667 0.2        0.25\n",
      " 0.33333333 0.5        0.11111111 0.125      0.14285714 0.16666667\n",
      " 0.2        0.25       0.33333333 0.11111111 0.125      0.14285714\n",
      " 0.16666667 0.2        0.25       0.33333333 0.11111111 0.125\n",
      " 0.14285714 0.16666667 0.2        0.11111111 0.125      0.14285714\n",
      " 0.16666667 0.2        0.25       0.33333333 0.5        0.11111111\n",
      " 0.125      0.14285714 0.16666667 0.2        0.25       0.33333333\n",
      " 0.5        0.11111111 0.125      0.14285714 0.16666667 0.2\n",
      " 0.25       0.33333333 0.5        1.         0.11111111 0.125\n",
      " 0.14285714 0.16666667 0.2       ]\n",
      "[ 0.  0.  0. -1.  1.  0.  0.  0.  0.  0. -1.  1.  0.  0.  0.  0.  0.  0.\n",
      " -1.  1.  0.  0.  0.  0.  0. -1.  1.  0.  0.  0.  0.  0. -1.  1.  0.  0.\n",
      "  0. -1.  1.  0.  0.  0.  0.  0.  0. -1.  1.  0.  0.  0.  0.  0.  0. -1.\n",
      "  1.  0.  0.  0.  0.  0.  0.  0. -1.  1.  0.  0.  0. -1.  1.]\n",
      "testing policy 1:randompi vs 2:randompi\n",
      "100/100 [==============================] - 0s 421us/step - draws: 0.0011 - wins1: 0.0056 - wins2: 0.0033 - win ratio: 1.5974          \n",
      "enumerating policy 1:randompi vs 2:randompi with EnumProc\n",
      "enumerating policy 1:lookaheadpi(randompi) vs 2:randompi with EnumProc\n",
      "enumerating policy 1:randompi vs 2:lookaheadpi(randompi) with EnumProc\n",
      "enumerating policy 1:lookaheadpi(randompi) vs 2:lookaheadpi(randompi) with EnumProc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy1</th>\n",
       "      <th>policy2</th>\n",
       "      <th>states1</th>\n",
       "      <th>states2</th>\n",
       "      <th>draws</th>\n",
       "      <th>wins1</th>\n",
       "      <th>wins2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>randompi</td>\n",
       "      <td>randompi</td>\n",
       "      <td>2739</td>\n",
       "      <td>2739</td>\n",
       "      <td>16</td>\n",
       "      <td>626</td>\n",
       "      <td>316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lookaheadpi(randompi)</td>\n",
       "      <td>randompi</td>\n",
       "      <td>2701</td>\n",
       "      <td>2581</td>\n",
       "      <td>16</td>\n",
       "      <td>604</td>\n",
       "      <td>316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>randompi</td>\n",
       "      <td>lookaheadpi(randompi)</td>\n",
       "      <td>2701</td>\n",
       "      <td>2733</td>\n",
       "      <td>16</td>\n",
       "      <td>620</td>\n",
       "      <td>316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lookaheadpi(randompi)</td>\n",
       "      <td>lookaheadpi(randompi)</td>\n",
       "      <td>2657</td>\n",
       "      <td>2569</td>\n",
       "      <td>16</td>\n",
       "      <td>604</td>\n",
       "      <td>316</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 policy1                policy2  states1  states2  draws  \\\n",
       "0               randompi               randompi     2739     2739     16   \n",
       "1  lookaheadpi(randompi)               randompi     2701     2581     16   \n",
       "2               randompi  lookaheadpi(randompi)     2701     2733     16   \n",
       "3  lookaheadpi(randompi)  lookaheadpi(randompi)     2657     2569     16   \n",
       "\n",
       "   wins1  wins2  \n",
       "0    626    316  \n",
       "1    604    316  \n",
       "2    620    316  \n",
       "3    604    316  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### ENVIRONMENT ###\n",
    "\n",
    "# 0 1 2\n",
    "# 3 4 5\n",
    "# 6 7 8\n",
    "\n",
    "winidx = [\n",
    "    [ [0,1,2],          [0,3,6], [0,4,8] ], # 0 - (0,0)\n",
    "    [ [0,1,2],          [1,4,7]          ], # 1 - (0,1)\n",
    "    [ [0,1,2], [2,4,6], [2,5,8]          ], # 2 - (0,2)\n",
    "    [ [3,4,5],          [0,3,6]          ], # 3 - (1,0)\n",
    "    [ [3,4,5], [2,4,6], [1,4,7], [0,4,8] ], # 4 - (1,1)\n",
    "    [ [3,4,5],          [2,5,8]          ], # 5 - (1,2)\n",
    "    [ [6,7,8], [2,4,6], [0,3,6],         ], # 6 - (2,0)\n",
    "    [ [6,7,8],          [1,4,7]          ], # 7 - (2,1)\n",
    "    [ [6,7,8],          [2,5,8], [0,4,8] ], # 8 - (2,2)\n",
    "]\n",
    "\n",
    "encoding = array([\n",
    "    [0,0], # 0\n",
    "    [0,1], # 1\n",
    "    [1,0], # 2\n",
    "]) \n",
    "\n",
    "def onehot(s): \n",
    "    return concatenate(encoding[s.astype(int)])\n",
    "\n",
    "def digits(s):\n",
    "    return ''.join(int64(s).ravel().astype(str))\n",
    "\n",
    "def other(p): \n",
    "    return (p%2)+1\n",
    "\n",
    "def player(s):\n",
    "    return other(count_nonzero(s))\n",
    "\n",
    "def action2xy(a):\n",
    "    x,y = unravel_index(a, (3,3))\n",
    "    return (x,y)\n",
    "\n",
    "def xy2action(x,y):\n",
    "    a = ravel_multi_index((x,y), (3,3))\n",
    "    return a\n",
    "\n",
    "def enum_lines(a):\n",
    "    return winidx[a]\n",
    "            \n",
    "def iswin(s,a):\n",
    "    p = s[a]\n",
    "    for line in enum_lines(a):\n",
    "        if all(s[line] == p):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def canwin(s,a,p):\n",
    "    a    = int(a)\n",
    "    s[a] = p\n",
    "    yes  = iswin(s,a)\n",
    "    s[a] = 0\n",
    "    return yes\n",
    "\n",
    "def hasbegun(s):\n",
    "    return count_nonzero(s) > 0\n",
    "\n",
    "def isover(s):\n",
    "    return count_nonzero(s) == len(s)\n",
    "\n",
    "def game(s=None,a=None):\n",
    "    winner = None\n",
    "    if s is None:\n",
    "        s = zeros(3*3)\n",
    "    elif a is None:\n",
    "        s = array(list(s)).astype(float)\n",
    "    else:\n",
    "        a    = int(a)\n",
    "        assert(s[a] == 0)\n",
    "        s    = copy(s)\n",
    "        p    = player(s)\n",
    "        s[a] = p\n",
    "        if iswin(s,a):\n",
    "            winner = p\n",
    "        elif isover(s):\n",
    "            winner = 0\n",
    "    return s,winner    \n",
    "\n",
    "def actions(s):\n",
    "    aa = argwhere(s == 0)\n",
    "    return concatenate(aa) if len(aa) > 0 else []\n",
    "\n",
    "def actionmask(aa):\n",
    "    aa       = aa.astype(int)\n",
    "    mask     = zeros(9)\n",
    "    mask[aa] = 1\n",
    "    return mask\n",
    "\n",
    "def selectaction(s,pi):\n",
    "    aa,pp = pi(s)\n",
    "    i     = choice(range(len(pp)), p=pp)\n",
    "    return aa[i],pp[i]    \n",
    "\n",
    "def getreward(agent, winner):\n",
    "    rival = other(agent)\n",
    "    if winner == agent:\n",
    "        return 1\n",
    "    if winner == rival:\n",
    "        return -1\n",
    "    return 0\n",
    "\n",
    "def getoutcome(ss,aa,pp,rr,discount):\n",
    "    outcome = {1:0,2:0}\n",
    "    n       = len(rr)\n",
    "    yy      = zeros_like(rr)\n",
    "    for i in reversed(range(n)):\n",
    "        actor          = player(ss[i])\n",
    "        _,winner       = game(ss[i],aa[i])      \n",
    "        if winner is not None:\n",
    "            outcome    = {1:0,2:0}\n",
    "        yy[i]          = rr[i] + outcome[actor]\n",
    "        outcome[actor] = yy[i] * discount\n",
    "    return yy\n",
    "\n",
    "def episode(policy, start=None):\n",
    "    n  = 0\n",
    "    ss = zeros((9,9))\n",
    "    aa = zeros((9))\n",
    "    pp = zeros((9))\n",
    "    s,winner = game() if start is None else (start,None)\n",
    "    while winner is None:\n",
    "        actor   = player(s)\n",
    "        rival   = other(actor) \n",
    "        a,p     = selectaction(s, policy[actor])\n",
    "        ss[n,:] = s\n",
    "        aa[n]   = a\n",
    "        pp[n]   = p\n",
    "        n      += 1     \n",
    "        s,winner = game(s, a)\n",
    "    ss = ss[0:n,:]\n",
    "    aa = aa[0:n]\n",
    "    pp = pp[0:n]\n",
    "    return winner,ss,aa,pp\n",
    "\n",
    "def winratio(wins, agent=1):\n",
    "    rival       = other(agent)\n",
    "    draw_score  = wins[0] * 0.5\n",
    "    agent_score = wins[agent] + draw_score\n",
    "    rival_score = wins[rival] + draw_score\n",
    "    ratio       = agent_score / (rival_score or 1)\n",
    "    return ratio\n",
    "\n",
    "def testgames(policy, iters=1000):\n",
    "    ratio    = 0\n",
    "    agent,*_ = policy.keys()\n",
    "    rival    = other(agent)\n",
    "    wins     = [0,0,0]\n",
    "    progbar  = Progbar(target=iters, stateful_metrics=['draws','wins1','wins2','win ratio'])\n",
    "    print(f'testing policy {agent}:{policy[agent].__name__} vs {rival}:{policy[rival].__name__}')\n",
    "    for i in range(iters):\n",
    "        winner,_,_,_  = episode(policy)\n",
    "        wins[winner] += 1\n",
    "        ratio         = winratio(wins,agent)\n",
    "        progbar.update(i+1, values=[\n",
    "            ('draws', wins[0]/10_000),\n",
    "            ('wins1', wins[1]/10_000),\n",
    "            ('wins2', wins[2]/10_000),\n",
    "            ('win ratio', ratio),             \n",
    "        ])    \n",
    "    return ratio\n",
    "\n",
    "def samplegames(policy, iters=100, start=None, progress=None):\n",
    "    m       = 0\n",
    "    sss     = zeros((9*iters,9))\n",
    "    aaa     = zeros((9*iters))\n",
    "    ppp     = zeros((9*iters))\n",
    "    rrr     = zeros((9*iters))\n",
    "    progbar = Progbar(target=iters, stateful_metrics=['total samples']) if progress else None\n",
    "    for i in range(iters):\n",
    "        winner,ss,aa,pp = episode(policy, start=start)\n",
    "        rr              = zeros((len(ss)))\n",
    "        rr[-1]          = getreward(player(ss[-1]), winner)\n",
    "        rr[-2]          = getreward(player(ss[-2]), winner)\n",
    "        d               = len(rr)\n",
    "        sss[m:m+d,:]    = ss\n",
    "        aaa[m:m+d]      = aa\n",
    "        ppp[m:m+d]      = pp\n",
    "        rrr[m:m+d]      = rr\n",
    "        m              += d\n",
    "        if progbar is not None:\n",
    "            progbar.update(i+1, values=[\n",
    "                ('samples per episode', d),\n",
    "                ('total samples', m),\n",
    "            ])    \n",
    "    return sss[0:m,:], aaa[0:m], ppp[0:m], rrr[0:m]\n",
    "\n",
    "class EnumProc:\n",
    "    def __init__(self):\n",
    "        self.__name__ = 'EnumProc'\n",
    "        self.states   = [0,0,0]\n",
    "        self.wins     = [0,0,0]\n",
    "    def __call__(self, s, winner):\n",
    "        p = player(s)\n",
    "        self.states[p] += 1\n",
    "        if winner is not None:\n",
    "            self.wins[winner] += 1\n",
    "    \n",
    "def enum_policy(policy, proc):\n",
    "    print(f'enumerating policy 1:{policy[1].__name__} vs 2:{policy[2].__name__} with {proc.__name__}')\n",
    "    visited = {}\n",
    "    def iter(s,w):\n",
    "        key = digits(s)\n",
    "        if key not in visited:\n",
    "            visited[key] = 0\n",
    "            proc(s,w)\n",
    "            if w is None:\n",
    "                actor = player(s)\n",
    "                aa,pp = policy[actor](s)\n",
    "                for a,p in zip(aa,pp):\n",
    "                    if p > 0:\n",
    "                        iter(*game(s,a))\n",
    "        visited[key] += 1\n",
    "    iter(*game())\n",
    "\n",
    "def enum_policies(policies):\n",
    "    policy1,policy2,states1,states2,draws,wins1,wins2 = [],[],[],[],[],[],[]\n",
    "    for policy in policies:\n",
    "        proc = EnumProc()\n",
    "        enum_policy(policy, proc)\n",
    "        policy1.append(policy[1].__name__)\n",
    "        policy2.append(policy[2].__name__)\n",
    "        states1.append(proc.states[1])\n",
    "        states2.append(proc.states[2])\n",
    "        draws.append(proc.wins[0])\n",
    "        wins1.append(proc.wins[1])\n",
    "        wins2.append(proc.wins[2])\n",
    "    return DataFrame(data={\n",
    "        'policy1': policy1,\n",
    "        'policy2': policy2,\n",
    "        'states1': states1,\n",
    "        'states2': states2,\n",
    "        'draws': draws,\n",
    "        'wins1': wins1,\n",
    "        'wins2': wins2\n",
    "    })    \n",
    "    \n",
    "def argsmax(values):\n",
    "    return unique(ravel(argwhere(values == max(values))))\n",
    "        \n",
    "def lookahead(s, aa):\n",
    "    actor = player(s)\n",
    "    ww    = [game(s,a)[1] for a in aa]\n",
    "    ii    = [i for i,w in enumerate(ww) if w == actor]\n",
    "#     if len(ii) == 0:\n",
    "#         rival = other(actor)\n",
    "#         ii    = [i for i,a in enumerate(aa) if canwin(s,a,rival)]\n",
    "    return ii\n",
    "    \n",
    "def uniformprob(aa,ii):\n",
    "    pp     = zeros_like(aa).astype(float)\n",
    "    pp[ii] = array(1/len(ii))\n",
    "    return pp\n",
    "\n",
    "def argmaxprob(qq):\n",
    "    ii = argsmax(qq)\n",
    "    pp = uniformprob(qq,ii)\n",
    "    return ravel(pp)\n",
    "\n",
    "def softmaxprob(zz):\n",
    "    max_z = max(zz)\n",
    "    num   = exp(zz - max_z) \n",
    "    den   = sum(num)\n",
    "    return num / den\n",
    "\n",
    "def randompi(s):\n",
    "    aa = actions(s)\n",
    "    n  = len(aa)\n",
    "    pp = array([1/n] * n)\n",
    "    return aa,pp\n",
    "\n",
    "def lookaheadpi(pi):\n",
    "    @rename(f'lookaheadpi({pi.__name__})')\n",
    "    def lookaheadpi(s):\n",
    "        aa = actions(s)\n",
    "        ii = lookahead(s, aa)\n",
    "        if len(ii) > 0: \n",
    "            pp = uniformprob(aa,ii)\n",
    "            return aa,pp\n",
    "        return pi(s)    \n",
    "    return lookaheadpi\n",
    "\n",
    "def maxpi(pi):\n",
    "    @rename(f'maxpi({pi.__name__})')\n",
    "    def maxpi(s):\n",
    "        aa,pp = pi(s)\n",
    "        pp    = argmaxprob(pp)\n",
    "        return aa,pp\n",
    "    return maxpi\n",
    "\n",
    "def percentilepi(pi, centile=90):\n",
    "    @rename(f'percentilepi({pi.__name__})')\n",
    "    def percentilepi(s):\n",
    "        aa,pp = pi(s)\n",
    "        th    = percentile(pp,centile)\n",
    "        ii    = argwhere(pp >= th)\n",
    "        pp    = uniformprob(aa,ii)\n",
    "        return aa,pp\n",
    "    return percentilepi\n",
    "\n",
    "def explorepi(pi, epsilon=0.1):\n",
    "    @rename(f'explorepi({pi.__name__},{epsilon})')\n",
    "    def explorepi(s):\n",
    "        if random() < epsilon:\n",
    "            return randompi(s)\n",
    "        return pi(s)\n",
    "    return explorepi\n",
    "\n",
    "def choicepi(*policies):\n",
    "    names = ','.join([pi.__name__ for pi in policies])\n",
    "    @rename(f'choicepi({names})')\n",
    "    def choicepi(s):\n",
    "        pi = choice(policies)\n",
    "        return pi(s)\n",
    "    return choicepi\n",
    "\n",
    "def switchpi(*policies):\n",
    "    pi    = choice(policies)\n",
    "    names = ','.join([pi.__name__ for pi in policies])\n",
    "    @rename(f'switchpi({names})')\n",
    "    def switchpi(s):\n",
    "        nonlocal pi\n",
    "        if count_nonzero(s) < 2:\n",
    "            pi = choice(policies)\n",
    "        return pi(s)\n",
    "    return switchpi\n",
    "\n",
    "seed(42)\n",
    "\n",
    "ss,aa,pp,rr = samplegames({ 1:randompi, 2:randompi }, iters=10, progress=True)\n",
    "print(ss)\n",
    "print(aa)\n",
    "print(pp)\n",
    "print(rr)\n",
    "\n",
    "testgames({ 1:randompi, 2:randompi },iters=TESTS)\n",
    "\n",
    "enum_policies([\n",
    "    {1:randompi,              2:randompi},\n",
    "    {1:lookaheadpi(randompi), 2:randompi},\n",
    "    {1:randompi,              2:lookaheadpi(randompi)},\n",
    "    {1:lookaheadpi(randompi), 2:lookaheadpi(randompi)},\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter Widgets: Simple game frontend to try agents right here in the notebook\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "play against lookaheadpi(randompi)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41ec57dfafaf4e0dba3c4c9b5dc3504f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Button(layout=Layout(height='40px', width='40px'), style=ButtonStyle(), tooltip=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### FRONTEND ###\n",
    "\n",
    "def play(policy):\n",
    "    state   = None\n",
    "    board   = None\n",
    "    agent   = None\n",
    "    rival   = None\n",
    "    \n",
    "    def moveagent(s, a):\n",
    "        aa,pp    = policy[agent](s)\n",
    "        i        = choice(range(len(pp)), p=pp)\n",
    "        a,p      = aa[i],pp[i]\n",
    "        s,winner = game(s, a)\n",
    "        return s,winner\n",
    "\n",
    "    def display_board(onclick):\n",
    "        board = []\n",
    "        for i in range(9):\n",
    "            btn = widgets.Button(\n",
    "                description  = '',\n",
    "                disabled     = False,\n",
    "                button_style = '', # 'success', 'info', 'warning', 'danger' or ''\n",
    "                tooltip      = 'Click me',\n",
    "                icon         = '',\n",
    "                layout       = Layout(width='40px', height='40px')\n",
    "            )\n",
    "            btn.action = i\n",
    "            btn.on_click(lambda btn: onclick(btn.action))\n",
    "            board.append(btn)\n",
    "        display(VBox([\n",
    "            HBox([board[0],board[1],board[2]]),\n",
    "            HBox([board[3],board[4],board[5]]),\n",
    "            HBox([board[6],board[7],board[8]])\n",
    "        ]))\n",
    "        return board\n",
    "        \n",
    "    def update_board(board, state):\n",
    "        chars = [' ', 'x', 'o']\n",
    "        state = state.astype(int)\n",
    "        for i in range(9):\n",
    "            board[i].description = chars[state[i]]\n",
    "\n",
    "    def gameturn(s=None, a=None):\n",
    "        s,winner = game(s,a)\n",
    "#         print(reshape(s, (3,3)))\n",
    "        update_board(board, s)\n",
    "        if winner is None and player(s) == agent:\n",
    "            s,winner = moveagent(s,a)\n",
    "            update_board(board, s)\n",
    "        if winner is not None:\n",
    "            msgs = ['DRAW','X WINS','O WINS']\n",
    "            print(msgs[winner])\n",
    "            for i in range(9):\n",
    "                board[i].disabled = True\n",
    "            play(policy)\n",
    "        return s,winner\n",
    "\n",
    "    def onclick(a): \n",
    "        nonlocal state\n",
    "        if (player(state) != agent) and (a in actions(state)):\n",
    "            state,winner = gameturn(state,a)\n",
    "\n",
    "    assert(1 in policy or 2 in policy)    \n",
    "    if 1 in policy:\n",
    "        agent,rival = 1,2\n",
    "    elif 2 in policy:\n",
    "        agent,rival = 2,1\n",
    "    print(f'play against {policy[agent].__name__}')\n",
    "    \n",
    "    board        = display_board(onclick=onclick)\n",
    "    state,winner = gameturn()\n",
    "\n",
    "play(policy={1:lookaheadpi(randompi)})\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabular Reinforcement Learning\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Programming: Action-value iteration\n",
    "Action-value iteration works by iteratively applying the Bellman optimality equation for $q_{\\ast}$ to a working action-value function, as an update rule, as shown below.\n",
    "\n",
    "$$\\large q_{\\ast}(s, a) = \\sum_{s', r} p(s', r | s, a)[r + \\gamma v_{\\ast}(s')]$$\n",
    "\n",
    "Alternatively we can express this equation in terms of $q_{\\ast}$ itself.\n",
    "\n",
    "$$\\large q_{\\ast}(s, a) = \\sum_{s', r} p(s', r | s, a)[r + \\gamma \\max_{a'} q_{\\ast}(s', a')]$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dptable: 5478\n",
      "testing policy 1:qpi(dptable) vs 2:lookaheadpi(randompi)\n",
      "100/100 [==============================] - 0s 1ms/step - draws: 0.0000e+00 - wins1: 0.0100 - wins2: 0.0000e+00 - win ratio: 100.0000\n",
      "testing policy 2:qpi(dptable) vs 1:lookaheadpi(randompi)\n",
      "100/100 [==============================] - 0s 1ms/step - draws: 9.0000e-04 - wins1: 0.0000e+00 - wins2: 0.0091 - win ratio: 21.2222\n",
      "testing policy 1:qpi(dptable) vs 2:qpi(dptable)\n",
      "100/100 [==============================] - 0s 2ms/step - draws: 0.0100 - wins1: 0.0000e+00 - wins2: 0.0000e+00 - win ratio: 1.0000\n",
      "enumerating policy 1:qpi(dptable) vs 2:lookaheadpi(randompi) with EnumProc\n",
      "enumerating policy 1:lookaheadpi(randompi) vs 2:qpi(dptable) with EnumProc\n",
      "enumerating policy 1:qpi(dptable) vs 2:qpi(dptable) with EnumProc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy1</th>\n",
       "      <th>policy2</th>\n",
       "      <th>states1</th>\n",
       "      <th>states2</th>\n",
       "      <th>draws</th>\n",
       "      <th>wins1</th>\n",
       "      <th>wins2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>qpi(dptable)</td>\n",
       "      <td>lookaheadpi(randompi)</td>\n",
       "      <td>1219</td>\n",
       "      <td>1004</td>\n",
       "      <td>7</td>\n",
       "      <td>554</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lookaheadpi(randompi)</td>\n",
       "      <td>qpi(dptable)</td>\n",
       "      <td>577</td>\n",
       "      <td>737</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>qpi(dptable)</td>\n",
       "      <td>qpi(dptable)</td>\n",
       "      <td>25</td>\n",
       "      <td>26</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 policy1                policy2  states1  states2  draws  \\\n",
       "0           qpi(dptable)  lookaheadpi(randompi)     1219     1004      7   \n",
       "1  lookaheadpi(randompi)           qpi(dptable)      577      737     16   \n",
       "2           qpi(dptable)           qpi(dptable)       25       26      7   \n",
       "\n",
       "   wins1  wins2  \n",
       "0    554      0  \n",
       "1      0    260  \n",
       "2      0      0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TABULAR-DYNAMIC-PROGRAMMING ###\n",
    "\n",
    "class qtable(dict):\n",
    "    def __init__(self, name):\n",
    "        self.__name__ = name\n",
    "    def __call__(self, s, a):\n",
    "        s,_ = game(s,a)\n",
    "        key = digits(s)\n",
    "        q   = self.get(key,0)\n",
    "        return q\n",
    "    def path(self):\n",
    "        return f'{DIR}/{self.__name__}.npy'\n",
    "    def save(self):\n",
    "        save(self.path(), self)\n",
    "    def load(self):\n",
    "        tmp = load(self.path(), allow_pickle=True)\n",
    "        self.update(tmp.item())\n",
    "    def exists(self):\n",
    "        return isfile(self.path())\n",
    "    \n",
    "def qpi(model):\n",
    "    @rename(f'qpi({model.__name__})')\n",
    "    def qpi(s):\n",
    "        aa = actions(s)\n",
    "        qq = array([model(s,a) for a in aa])\n",
    "        pp = argmaxprob(qq)\n",
    "        return aa,pp\n",
    "    return qpi\n",
    "\n",
    "def enum_states(proc):\n",
    "    visited = {}\n",
    "    def iter(s,w):\n",
    "        key = digits(s)\n",
    "        if key not in visited:\n",
    "            visited[key] = 1\n",
    "            proc(s,w)\n",
    "            if w is None:\n",
    "                for a in actions(s):\n",
    "                    iter(*game(s,a))\n",
    "    iter(*game())\n",
    "\n",
    "def iter_qvalue(table, iters=10, target=1e-8):    \n",
    "    def eval_afterstate(after_state, winner):    \n",
    "        new_value = 0\n",
    "        rival     = player(after_state)\n",
    "        agent     = other(rival)\n",
    "        if winner is None: \n",
    "            rival_actions = actions(after_state)\n",
    "            rival_prob    = 1 / len(rival_actions)\n",
    "            for rival_action in rival_actions:\n",
    "                next_state,winner = game(after_state, rival_action)\n",
    "                if winner is None: \n",
    "                    max_next_value = max([table(next_state,a) for a in actions(next_state)])\n",
    "                    new_value     += rival_prob * (0 + max_next_value)\n",
    "                else:\n",
    "                    assert(winner != agent)\n",
    "                    reward     = -1 if winner == rival else 0\n",
    "                    new_value += rival_prob * (reward + 0)\n",
    "                    if winner == rival:              # commenting this  \n",
    "                        new_value = 1 * (reward + 0) # prevents player2\n",
    "                        break                        # from learning optimal policy\n",
    "        else:\n",
    "            assert(winner != rival)\n",
    "            reward    = 1 if winner == agent else 0\n",
    "            new_value = 1 * (reward + 0)\n",
    "        key        = digits(after_state)\n",
    "        old_value  = table.get(key, 0)\n",
    "        table[key] = new_value\n",
    "        return abs(old_value - new_value)\n",
    "    delta = 0\n",
    "    def proc(state, winner):\n",
    "        nonlocal delta\n",
    "        delta = max(delta, eval_afterstate(state, winner))\n",
    "    progbar = Progbar(target=iters, stateful_metrics=['delta'])\n",
    "    for i in range(iters):\n",
    "        delta = 0\n",
    "        enum_states(proc)\n",
    "        progbar.update(i+1, values=[('delta',  delta)]) \n",
    "        if delta <= target:\n",
    "            break\n",
    "    print('\\n')\n",
    "         \n",
    "dptable = qtable('dptable')  \n",
    "if dptable.exists():\n",
    "    dptable.load()\n",
    "else:\n",
    "    iter_qvalue(dptable, iters=100, target=1e-08)\n",
    "    dptable.save()\n",
    "print('dptable:', len(dptable))\n",
    "mtime2str(dptable.path())\n",
    "\n",
    "agentpi = qpi(dptable) \n",
    "rivalpi = lookaheadpi(randompi)\n",
    "\n",
    "testgames({1:agentpi, 2:rivalpi}, iters=TESTS)\n",
    "testgames({2:agentpi, 1:rivalpi}, iters=TESTS)\n",
    "testgames({1:agentpi, 2:agentpi}, iters=TESTS)\n",
    "\n",
    "enum_policies([\n",
    "    {1:agentpi, 2:rivalpi},\n",
    "    {1:rivalpi, 2:agentpi},\n",
    "    {1:agentpi, 2:agentpi},\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "play against qpi(dptable)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5794b9a1a6a64ddc972b7779ff06b022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Button(layout=Layout(height='40px', width='40px'), style=ButtonStyle(), tooltip=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "play(policy={2:agentpi})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte-Carlo RL:\n",
    "\n",
    "$$\\large V(S_t) = V(S_t) + \\alpha [ G_t - V(S_t) ]$$\n",
    "\n",
    "$$\\large Q(S_t,A_t) = Q(S_t,A_t) + \\alpha [ G_t - Q(S_t,A_t) ]$$\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TABULAR-MONTE-CARLO ###\n",
    "\n",
    "def iter_under_policy(policy, iters=100, games=1, rates={0:0.1}, epsilons={0:0.1}, proc=print):\n",
    "    print(f'iterating under policy {1}:{policy[1].__name__} vs {2}:{policy[2].__name__}')\n",
    "    progbar  = Progbar(target=iters, stateful_metrics=['rate', 'epsilon'])\n",
    "    errors   = zeros(iters) \n",
    "    rewards  = [None, zeros(iters), zeros(iters)]\n",
    "    for i in range(iters):\n",
    "        if i in rates:\n",
    "            rate = rates[i]\n",
    "        if i in epsilons:\n",
    "            epsilon = epsilons[i]\n",
    "            policy2 = {\n",
    "                1:explorepi(policy[1], epsilon),\n",
    "                2:explorepi(policy[2], epsilon),\n",
    "            }\n",
    "        ss,aa,pp,rr = samplegames(policy=policy2, iters=games)\n",
    "        errors[i]   = proc(ss,aa,pp,rr,rate=rate) or 0\n",
    "        for s,r in zip(ss,rr):\n",
    "            actor = player(s)\n",
    "            rewards[actor][i] += r/games \n",
    "        values = [\n",
    "            ('rate',      rate),\n",
    "            ('epsilon',   epsilon), \n",
    "            ('error',     errors[i]),\n",
    "            ('reward[X]', rewards[1][i]),\n",
    "            ('reward[O]', rewards[2][i])\n",
    "        ]\n",
    "        progbar.update(i+1, values)\n",
    "#         progress(f'{ipynb}, iter {i} of {iters}, {dict(values)}')    \n",
    "    figure()\n",
    "    plot(errors,'r')\n",
    "    plot(rewards[1])\n",
    "    plot(rewards[2])\n",
    "    title('objective history')\n",
    "    ylabel('objective')\n",
    "    xlabel(f'games x{games}')\n",
    "    legend(['error', 'reward[X]', 'reward[O]'], loc='upper left')\n",
    "    savefig(f'{DIR}/{policy[1].__name__}-vs-{policy[2].__name__}.png')\n",
    "    show()     \n",
    "\n",
    "def mctrain(table, discount=0.99, iters=100, games=1, rates={0:0.1}, epsilons={0:0.1}):\n",
    "    def mcproc(ss,aa,pp,rr,rate=0.1):\n",
    "        error = 0\n",
    "        yy    = getoutcome(ss,aa,pp,rr,discount=discount)\n",
    "        for s,a,y in zip(ss,aa,yy):\n",
    "            key         = digits(game(s,a)[0])\n",
    "            q           = table.get(key, 0)\n",
    "            diff        = y - q\n",
    "            table[key]  = q + rate*diff\n",
    "            error      += abs(diff)\n",
    "        return error / len(ss)\n",
    "    agentpi = lookaheadpi(qpi(table))\n",
    "    policy  = {1:agentpi,2:agentpi}\n",
    "    iter_under_policy(policy, iters=iters, games=games, rates=rates, epsilons=epsilons, proc=mcproc)\n",
    "        \n",
    "seed(42)\n",
    "mctable = qtable('mctable')   \n",
    "if mctable.exists():\n",
    "    mctable.load()\n",
    "else:\n",
    "    mctrain(mctable, \n",
    "            discount = 0.99, \n",
    "            iters    = 10_000, \n",
    "            games    = 10, \n",
    "            rates    = { 0:0.1 }, \n",
    "            epsilons = { 0:0.1, 5000:0.05 })\n",
    "    mctable.save()\n",
    "print('mctable:', len(mctable))\n",
    "mtime2str(mctable.path())\n",
    "          \n",
    "agentpi = qpi(mctable) \n",
    "rivalpi = lookaheadpi(randompi)\n",
    "\n",
    "testgames({1:agentpi, 2:rivalpi}, iters=TESTS)\n",
    "testgames({2:agentpi, 1:rivalpi}, iters=TESTS)\n",
    "testgames({1:agentpi, 2:agentpi}, iters=TESTS)\n",
    "\n",
    "enum_policies([\n",
    "    {1:agentpi, 2:rivalpi},\n",
    "    {1:rivalpi, 2:agentpi},\n",
    "    {1:agentpi, 2:agentpi},\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play(policy={2:agentpi})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal difference: Q-Learning\n",
    "\n",
    "$$\\large V(S_t) = V(S_t) + \\alpha [ R_{t+1} + \\gamma V(S_{t+1}) - V(S_t) ]$$\n",
    "\n",
    "$$\\large Q(S_t, A_t) = Q(S_t, A_t) + \\alpha [ R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) ]$$\n",
    "\n",
    "$$\\large Q(S_t, A_t) = Q(S_t, A_t) + \\alpha [ R_{t+1} + \\gamma \\max_{a} Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) ]$$\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TABULAR-TEMPORAL-DIFFERENCE ###\n",
    "        \n",
    "def tdtrain(table, discount=0.99, iters=100, games=1, rates={0:0.1}, epsilons={0:0.1}):\n",
    "    def tdproc(ss,aa,pp,rr,rate=0.1):\n",
    "        error = 0\n",
    "        n     = len(ss)\n",
    "        for j in reversed(range(n)):\n",
    "            reward    = rr[j]\n",
    "            after_s,_ = game(ss[j],aa[j])\n",
    "            key       = digits(after_s)\n",
    "            q         = table.get(key, 0)\n",
    "            if j+1 < n and all(after_s == ss[j+1]):\n",
    "                next_s,_   = game(ss[j+1],aa[j+1])\n",
    "                qq         = [table(next_s,a) for a in actions(next_s)]\n",
    "                max_next_q = max(qq) if len(qq) > 0 else 0\n",
    "            else:    \n",
    "                max_next_q = 0\n",
    "            diff       = reward + discount*max_next_q - q\n",
    "            table[key] = q + rate*diff                \n",
    "            error     += abs(diff)\n",
    "        return error / n\n",
    "    agentpi = lookaheadpi(qpi(table))\n",
    "    policy  = {1:agentpi,2:agentpi}\n",
    "    iter_under_policy(policy, iters=iters, games=games, rates=rates, epsilons=epsilons, proc=tdproc)\n",
    "        \n",
    "seed(42)\n",
    "tdtable = qtable('tdtable')   \n",
    "if tdtable.exists():\n",
    "    tdtable.load()\n",
    "else:\n",
    "    tdtrain(tdtable, \n",
    "        discount = 0.99, \n",
    "        iters    = 10_000, \n",
    "        games    = 30, \n",
    "        rates    = {0:0.1, 3_000:0.01, 7_000:0.001}, \n",
    "        epsilons = {0:0.1, 5_000: 0.05})\n",
    "    tdtable.save()\n",
    "print('tdtable:', len(tdtable))\n",
    "mtime2str(tdtable.path())\n",
    "      \n",
    "agentpi = qpi(tdtable) \n",
    "rivalpi = lookaheadpi(randompi)\n",
    "\n",
    "testgames({1:agentpi, 2:rivalpi}, iters=TESTS)\n",
    "testgames({2:agentpi, 1:rivalpi}, iters=TESTS)\n",
    "testgames({1:agentpi, 2:agentpi}, iters=TESTS)\n",
    "\n",
    "enum_policies([\n",
    "    {1:agentpi, 2:rivalpi},\n",
    "    {1:rivalpi, 2:agentpi},\n",
    "    {1:agentpi, 2:agentpi},\n",
    "])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play(policy={2:agentpi})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Tree Search\n",
    "\n",
    "<img align=\"left\" src=\"MCTS.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\large UCT(S,a) = \\frac{Q(S_a)}{N(S_a)} + \\alpha*\\sqrt{\\frac{2\\ln{N(S)}}{N(S_a)}}$$\n",
    "\n",
    "$$\\large PUCT(S,a) = \\frac{Q(S_a)}{N(S_a)} + \\alpha*P(S,a)\\sqrt{\\frac{\\sum_b{N(S_b)}}{(1+N(S_a))}}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MONTE-CARLO-TREE-SEARCH ###\n",
    "\n",
    "class mctstable(qtable):\n",
    "    def __init__(self, name):\n",
    "        self.__name__ = name\n",
    "        self.notfound = None\n",
    "    def __call__(self, s, a):\n",
    "        s,_  = game(s,a)\n",
    "        key  = digits(s)\n",
    "        if key in self:\n",
    "            uid = self[key]\n",
    "            return self[uid]['value']\n",
    "        if self.notfound is not None:\n",
    "            self.notfound.add(key)\n",
    "        return 0   \n",
    "    \n",
    "def rot0(M):     return M\n",
    "def rot180(M):   return rot90(M,2)\n",
    "def rot270(M):   return rot90(M,3)\n",
    "def fliptrbl(M): return transpose(M)\n",
    "def fliptlbr(M): return transpose(rot90(M,2))\n",
    "\n",
    "rotations = [ rot0,   rot90,  rot180,   rot270   ]\n",
    "flips     = [ fliplr, flipud, fliptlbr, fliptrbl ]\n",
    "\n",
    "def map_rotations_and_flips():\n",
    "    statemap  = {}\n",
    "    actionmap = {}\n",
    "    state   = array(arange(9))\n",
    "    for f in rotations+flips:\n",
    "        s1 = ravel(f(reshape(state,(3,3))))\n",
    "        s2 = zeros_like(s1)\n",
    "        for i,x in enumerate(s1):\n",
    "            s2[x] = i;\n",
    "        statemap [f.__name__] = s1\n",
    "        actionmap[f.__name__] = s2\n",
    "    return statemap,actionmap\n",
    "    \n",
    "statemap,actionmap = map_rotations_and_flips()\n",
    "for f in rotations+flips:\n",
    "    print(f.__name__, statemap[f.__name__], actionmap[f.__name__])\n",
    "print()\n",
    "    \n",
    "def rotate_and_flip(x):\n",
    "    if isscalar(x): \n",
    "        x = int(x)\n",
    "        return array([ idx[x] for idx in actionmap.values() ])\n",
    "    else: \n",
    "        return array([ x[idx] for idx in statemap.values()  ])\n",
    "    \n",
    "def getstats(table, state):\n",
    "    key = digits(state)    \n",
    "    if key not in table:\n",
    "        uid        = str(guid())\n",
    "        table[uid] = { 'value': 0, 'visits': 0 }\n",
    "        for s in rotate_and_flip(state):\n",
    "            k = digits(s)\n",
    "            if k not in table:\n",
    "                table[k] = uid\n",
    "    uid = table[key]\n",
    "    return table[uid]\n",
    "\n",
    "def makenode(table, state, parent=None, terminal=False):\n",
    "    return {'parent'   : parent,\n",
    "            'stats'    : getstats(table, state),\n",
    "            'branches' : { a:None for a in actions(state) } if not terminal else None\n",
    "           }\n",
    "\n",
    "def isterminal(node):\n",
    "    return node['branches'] is None\n",
    "\n",
    "def isexpanded(node):\n",
    "    if isterminal(node):\n",
    "        return False\n",
    "    for child in node['branches'].values():\n",
    "        if child is None:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def backpropagate(node, reward):\n",
    "    while node is not None:\n",
    "        stats            = node['stats']\n",
    "        stats['visits'] += 1\n",
    "        stats['value']  += (reward - stats['value']) / stats['visits']\n",
    "        node             = node['parent']\n",
    "        reward           = -reward\n",
    "\n",
    "def expandbranch(table, node, state):\n",
    "    assert(not isterminal(node))\n",
    "    assert(not isexpanded(node))\n",
    "    branches          = node['branches']\n",
    "    actor             = player(state)\n",
    "    actions           = list(branches.keys())\n",
    "    unknown           = [ a for a in actions if branches[a] is None ]\n",
    "    action            = choice(unknown)\n",
    "    next_state,winner = game(state,action)\n",
    "    child             = makenode(table, next_state, parent=node, terminal=winner is not None)\n",
    "    branches[action]  = child\n",
    "    if isterminal(child):\n",
    "        reward = getreward(actor, winner) \n",
    "        backpropagate(child, reward)\n",
    "    return child,next_state\n",
    "                \n",
    "def ucb1(node, action, exploration): \n",
    "    n     = node['stats']['visits']\n",
    "    child = node['branches'][action]\n",
    "    nj    = child['stats']['visits']\n",
    "    xj    = child['stats']['value']\n",
    "    if exploration == 0:\n",
    "        return xj\n",
    "    if n == 0 or nj == 0:\n",
    "        return float('inf')\n",
    "    return xj + exploration*sqrt(2*log(n)/nj)\n",
    "\n",
    "def selectbranchpi(node, exploration=0):\n",
    "    assert(isexpanded(node))\n",
    "    actions  = list(node['branches'].keys())\n",
    "    values   = array([ ucb1(node,a,exploration) for a in actions ])\n",
    "    probs    = argmaxprob(values)\n",
    "    return actions,probs\n",
    "\n",
    "def selectbranch(node, state, exploration):\n",
    "    actions,probs = selectbranchpi(node, exploration)\n",
    "    action        = choice(actions, p=probs)\n",
    "    node          = node['branches'][action]\n",
    "    state,_       = game(state, action)\n",
    "    return node,state\n",
    "\n",
    "def selectnode(table, node, state, exploration):\n",
    "    while isexpanded(node):\n",
    "        node,state = selectbranch(node, state, exploration)\n",
    "    if not isterminal(node):\n",
    "        node,state = expandbranch(table, node, state)\n",
    "    return node,state\n",
    "\n",
    "def simulatefrom(leaf, state, policy):\n",
    "    if isterminal(leaf):\n",
    "        return leaf['stats']['value']\n",
    "    actor        = player(state)\n",
    "    winner,_,_,_ = episode(policy, start=state)\n",
    "    reward       = getreward(actor, winner)\n",
    "    return reward \n",
    "    \n",
    "def search(table, node, state, policy, exploration=1):\n",
    "    leaf,state = selectnode(table, node, state, exploration)\n",
    "    reward     = simulatefrom(leaf, state, policy)\n",
    "    backpropagate(leaf, reward)\n",
    "    return reward\n",
    "\n",
    "def searchpi(table, timeout=1, policy={1:randompi,2:randompi}):\n",
    "    @rename(f'searchpi({len(table)},{timeout})')\n",
    "    def searchpi(s):\n",
    "        t    = time()\n",
    "        node = makenode(table, s)\n",
    "        search(table, node, s, policy)\n",
    "        while time() - t < timeout:\n",
    "            search(table, node, s, policy)\n",
    "        aa,pp = selectbranchpi(node, exploration=0)\n",
    "        return aa,pp\n",
    "    return searchpi\n",
    "\n",
    "def train(table, policy, iters, exploration=1):\n",
    "    state    = game()[0]\n",
    "    root     = makenode(table, state)\n",
    "    progbar  = Progbar(target=iters, stateful_metrics=[])\n",
    "    progress = PROGRESS(path=f'{DIR}/progress.log')\n",
    "    for i in range(progbar.target):\n",
    "        reward = search(table, root, state, policy, exploration)\n",
    "        size   = len(table) \n",
    "        progbar.update(i+1, values=[('reward',reward),('size',size)])\n",
    "        progress(f'{ipynb}, iter {i+1} of {iters}, reward {reward}, size {size}')   \n",
    "\n",
    "seed(41)   \n",
    "          \n",
    "playoutpi = switchpi(randompi, lookaheadpi(randompi)) \n",
    "          \n",
    "mctsdata = mctstable('mctsdata')      \n",
    "policy   = {1:playoutpi,2:playoutpi}\n",
    "if mctsdata.exists():\n",
    "    mctsdata.load()\n",
    "else:\n",
    "    train(mctsdata, policy, iters=500_000, exploration=10)\n",
    "    mctsdata.save()\n",
    "mtime2str(mctsdata.path())\n",
    "          \n",
    "agentpi = searchpi(mctsdata,0.1)\n",
    "rivalpi = lookaheadpi(randompi)        \n",
    "ratio1  = testgames({1:agentpi, 2:rivalpi}, iters=10)          \n",
    "ratio2  = testgames({2:agentpi, 1:rivalpi}, iters=10)          \n",
    "\n",
    "mctsdata.notfound = set() \n",
    "agentpi = qpi(mctsdata)\n",
    "enum_policies([\n",
    "    {1:agentpi, 2:randompi},\n",
    "    {1:agentpi, 2:lookaheadpi(randompi)},\n",
    "    {1:agentpi, 2:qpi(dptable)},\n",
    "    {1:agentpi, 2:qpi(mctable)},\n",
    "    {1:agentpi, 2:qpi(tdtable)},\n",
    "    {2:agentpi, 1:randompi},\n",
    "    {2:agentpi, 1:lookaheadpi(randompi)},\n",
    "    {2:agentpi, 1:qpi(dptable)},\n",
    "    {2:agentpi, 1:qpi(mctable)},\n",
    "    {2:agentpi, 1:qpi(tdtable)},\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(mctsdata.notfound))\n",
    "pprint(mctsdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play(policy={2:searchpi(mctsdata,1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximate Reinforcement Learning:  Neural networks\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradients: REINFORCE\n",
    "\n",
    "$$\\large \\nabla J(\\theta) \\propto \\sum_s \\mu(s) \\sum_a q_\\pi(s,a) \\nabla \\pi(a|s, \\theta)$$\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### POLICY-GRADIENTS ###\n",
    "\n",
    "class pimodel(Model):\n",
    "    def __init__(self, name, *nn):\n",
    "        inf  = tf.constant(np.finfo(np.float32).min)\n",
    "        mask = Input((nn[-1],)) \n",
    "        def maskedsoftmax(x):\n",
    "            ones     = tf.ones_like(x)\n",
    "            boolmask = tf.cast(mask, dtype=tf.dtypes.bool)\n",
    "            masked_x = tf.where(boolmask, x, ones*inf)\n",
    "            return softmax(masked_x)\n",
    "        x     = Input((nn[0],))\n",
    "        y     = x\n",
    "        for n in nn[1:-1]:\n",
    "            y = Dense(n, activation='relu', trainable=True)(y)\n",
    "        y     = Dense(nn[-1], trainable=True)(y)\n",
    "        z     = Activation(maskedsoftmax)(y)\n",
    "        super(pimodel, self).__init__(inputs=[x,mask], outputs=z, name=name)\n",
    "        optimizer = Adam(lr=0.001, clipnorm=1.0)\n",
    "        self.compile(loss=categorical_crossentropy, optimizer=optimizer)     \n",
    "        self.__name__ = name      \n",
    "    def __call__(self, s):\n",
    "        aa   = actions(s)\n",
    "        xx   = array([onehot(s)]) \n",
    "        mask = array([actionmask(aa)])\n",
    "        zz   = self.predict([xx,mask])\n",
    "        pp   = ravel(zz)[aa]\n",
    "        return aa,pp\n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "    def path(self):\n",
    "        return f'{DIR}/{self.__name__}.npy'\n",
    "    def save(self):\n",
    "        save(self.path(), self.get_weights())\n",
    "    def load(self):\n",
    "        weights = load(self.path())\n",
    "        self.set_weights(weights)\n",
    "    def exists(self):\n",
    "        return isfile(self.path())\n",
    "\n",
    "def train(models, discount=0.99, iters=100, games=1, rates={0:0.1}, epsilons={0:0.1}):\n",
    "    avg_y         = 0\n",
    "    agent,*_      = models.keys()\n",
    "    rival         = other(agent)\n",
    "    model         = models[agent]\n",
    "    policy        = {}    \n",
    "    policy[agent] = model\n",
    "    policy[rival] = models[rival] if rival in models else lookaheadpi(randompi)\n",
    "    def proc(ss,aa,pp,rr,rate=0.1):\n",
    "        K.set_value(model.optimizer.lr, rate)\n",
    "        yy       = getoutcome(ss,aa,pp,rr,discount=discount)\n",
    "        ii       = [i for i,s in enumerate(ss) if agent == player(s)] \n",
    "        ss,aa,yy = ss[ii],aa[ii],yy[ii]\n",
    "        xx       = array([onehot(s) for s in ss])\n",
    "        mask     = array([actionmask(actions(s)) for s in ss])\n",
    "        ohaa     = to_categorical(aa,9)\n",
    "        error    = model.train_on_batch(x=[xx,mask], y=ohaa, sample_weight=yy, reset_metrics=False)\n",
    "        return error\n",
    "    iter_under_policy(policy, iters=iters, games=games, rates=rates, epsilons=epsilons, proc=proc)\n",
    "                \n",
    "seed(42)\n",
    "pimodel1 = pimodel('pimodel1',18,32,9)  \n",
    "print(pimodel1.summary())\n",
    "\n",
    "if pimodel1.exists():\n",
    "    pimodel1.load()\n",
    "else:\n",
    "    train({1:pimodel1}, \n",
    "        discount = 0.99, \n",
    "        iters    = 300, \n",
    "        games    = 20, \n",
    "        rates    = {0:0.01, 200:0.001}, \n",
    "        epsilons = {0:0.1, 1000: 0.1})\n",
    "    pimodel1.save()\n",
    "mtime2str(pimodel1.path())\n",
    "\n",
    "agentpi = pimodel1\n",
    "rivalpi = lookaheadpi(randompi)\n",
    "testgames({1:agentpi, 2:rivalpi}, iters=TESTS)\n",
    "\n",
    "agentpi = maxpi(pimodel1)\n",
    "rivalpi = lookaheadpi(randompi)\n",
    "testgames  ({1:agentpi, 2:rivalpi}, iters=TESTS)\n",
    "\n",
    "enum_policies([\n",
    "    {1:agentpi, 2:rivalpi},\n",
    "    {1:agentpi, 2:qpi(dptable)},\n",
    "    {1:agentpi, 2:qpi(mctable)},\n",
    "    {1:agentpi, 2:qpi(tdtable)},\n",
    "    {1:agentpi, 2:qpi(mctsdata)},\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play(policy={1:maxpi(pimodel1)}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### POLICY-GRADIENTS PLAYER2 ###\n",
    "\n",
    "seed(1)\n",
    "pimodel2 = pimodel('pimodel2',18,32,9)  \n",
    "print(pimodel2.summary())\n",
    "\n",
    "allpi = choicepi(randompi,lookaheadpi(randompi),qpi(dptable),qpi(mctable),qpi(tdtable))\n",
    "\n",
    "if pimodel2.exists():\n",
    "    pimodel2.load()\n",
    "else:\n",
    "    train({2:pimodel2,1:allpi}, \n",
    "        discount = 0.99, \n",
    "        iters    = 100_000, \n",
    "        games    = 30, \n",
    "        rates    = {0:0.01, 30_000:0.001, 70_000:0.0001}, \n",
    "        epsilons = {0:0.0}\n",
    "    )\n",
    "    pimodel2.save()\n",
    "mtime2str(pimodel2.path())\n",
    "\n",
    "agentpi = pimodel2\n",
    "rivalpi = lookaheadpi(randompi)\n",
    "testgames({2:agentpi, 1:rivalpi}, iters=TESTS)\n",
    "\n",
    "agentpi = maxpi(pimodel2)\n",
    "rivalpi = lookaheadpi(randompi)\n",
    "testgames  ({2:agentpi, 1:rivalpi}, iters=TESTS)\n",
    "\n",
    "enum_policies([\n",
    "    {2:agentpi, 1:lookaheadpi(randompi)},\n",
    "    {2:agentpi, 1:qpi(dptable)},\n",
    "    {2:agentpi, 1:qpi(mctable)},\n",
    "    {2:agentpi, 1:qpi(tdtable)},\n",
    "    {2:agentpi, 1:qpi(mctsdata)},\n",
    "    {2:agentpi, 1:maxpi(pimodel1)},\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play(policy={2:maxpi(pimodel2)}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximate Q-Learning\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### APPROXIMATE-Q-LEARNING ###\n",
    "\n",
    "class qmodel(Model):\n",
    "    def __init__(self, name, *nn, activation=None, buffer=100, batch=10):\n",
    "        def clone():\n",
    "            clone = qmodel(name,*nn,activation=activation)\n",
    "            clone.set_weights(self.get_weights())\n",
    "            return clone\n",
    "        x     = Input((nn[0],))\n",
    "        y     = x\n",
    "        for n in nn[1:-1]:\n",
    "            y = Dense(n, activation=activation)(y)\n",
    "        z     = Dense(nn[-1], activation='tanh')(y)\n",
    "        super(qmodel, self).__init__(inputs=x, outputs=z, name=name)\n",
    "        optimizer = Adam(lr=0.001, clipnorm=1.0)\n",
    "        self.compile(loss=mse, optimizer=optimizer) \n",
    "        self.__name__ = name        \n",
    "        self.buffer   = deque(maxlen=buffer*9) \n",
    "        self.batch    = batch*9\n",
    "        self.clone    = clone\n",
    "    def __call__(self, s, logits=False):\n",
    "        aa   = actions(s)\n",
    "        xx   = array([onehot(s)])\n",
    "        zz   = self.predict(xx)\n",
    "        qq   = ravel(zz)[aa]\n",
    "        if logits:\n",
    "            return aa,qq\n",
    "        pp   = softmaxprob(qq)\n",
    "        return aa,pp    \n",
    "    def push(self, xx, yy):\n",
    "        for x,y in zip(xx,yy):\n",
    "            self.buffer.append((x,y))\n",
    "    def pop(self):\n",
    "        n  = len(self.buffer)\n",
    "        ii = choice(arange(n), size=min(self.batch,n))\n",
    "        xx = array([self.buffer[i][0] for i in ii])\n",
    "        yy = array([self.buffer[i][1] for i in ii])\n",
    "        return xx,yy\n",
    "    def exchange(self, xx, yy):\n",
    "        self.push(xx, yy)\n",
    "        return self.pop()\n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "    def path(self):\n",
    "        return f'{DIR}/{self.__name__}.npy'\n",
    "    def save(self):\n",
    "        save(self.path(), self.get_weights())\n",
    "    def load(self):\n",
    "        weights = load(self.path())\n",
    "        self.set_weights(weights)\n",
    "    def exists(self):\n",
    "        return isfile(self.path())\n",
    "    \n",
    "def train(models, discount=0.9, iters=100, games=10, rates={0:0.1}, epsilons={0:0.1}):\n",
    "    agent,*_      = models.keys()\n",
    "    rival         = other(agent)\n",
    "    model         = models[agent]\n",
    "    policy        = {}    \n",
    "    policy[agent] = model\n",
    "    policy[rival] = models[rival] if rival in models else lookaheadpi(randompi)\n",
    "    def mcproc(ss,aa,pp,rr,rate=0.1):\n",
    "        K.set_value(model.optimizer.lr, rate)\n",
    "        yy = getoutcome(ss,aa,pp,rr,discount=discount)\n",
    "        xx = array([onehot(s) for s in ss])\n",
    "        zz = model.predict(xx)\n",
    "#         mask = array([actionmask(actions(s)) for s in ss])\n",
    "#         zz  *= mask\n",
    "        for z,a,y in zip(zz,aa,yy):\n",
    "            z[int(a)] = y\n",
    "        yy = zz\n",
    "        xx,yy = model.exchange(xx,yy)\n",
    "        error = model.train_on_batch(x=xx, y=yy, reset_metrics=False)\n",
    "        return error\n",
    "    iter_under_policy(policy, iters=iters, games=games, rates=rates, epsilons=epsilons, proc=mcproc)\n",
    "        \n",
    "seed(42)\n",
    "qmodelxo = qmodel('qmodelxo',18,36,9,activation='tanh',buffer=100,batch=20)  \n",
    "print(qmodelxo.summary())\n",
    "\n",
    "if qmodelxo.exists():\n",
    "    qmodelxo.load()\n",
    "else:\n",
    "    train({1:qmodelxo,2:qmodelxo}, \n",
    "          discount = 0.9, \n",
    "          iters    = 10_000, \n",
    "          games    = 10, \n",
    "          rates    = {0:0.1, 3_000:0.05, 5_000:0.01, 10_000:0.005, 15_000:0.001}, \n",
    "          epsilons = {0:0.1})\n",
    "    qmodelxo.save()\n",
    "mtime2str(qmodelxo.path())\n",
    "\n",
    "agentpi = maxpi(qmodelxo) \n",
    "rivalpi = lookaheadpi(randompi)\n",
    "\n",
    "testgames({1:agentpi, 2:rivalpi}, iters=TESTS)\n",
    "testgames({2:agentpi, 1:rivalpi}, iters=TESTS)\n",
    "\n",
    "enum_policies([\n",
    "    {1:agentpi, 2:rivalpi},\n",
    "    {1:agentpi, 2:qpi(dptable)},\n",
    "    {1:agentpi, 2:qpi(mctable)},\n",
    "    {1:agentpi, 2:qpi(tdtable)},\n",
    "    {1:agentpi, 2:qpi(mctsdata)},\n",
    "    {1:agentpi, 2:maxpi(pimodel2)},\n",
    "    {2:agentpi, 1:rivalpi},\n",
    "    {2:agentpi, 1:qpi(dptable)},\n",
    "    {2:agentpi, 1:qpi(mctable)},\n",
    "    {2:agentpi, 1:qpi(tdtable)},\n",
    "    {2:agentpi, 1:qpi(mctsdata)},\n",
    "    {2:agentpi, 1:maxpi(pimodel1)},\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play(policy={2:maxpi(qmodelxo)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalable Approach: Monte Carlo Tree Search and Deep Q-Network\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MCTS & DQN ###\n",
    "\n",
    "def makenode(value=0, parent=None, terminal=False):\n",
    "    return {'value'    : value,\n",
    "            'visits'   : 0,\n",
    "            'parent'   : parent,\n",
    "            'branches' : {} if not terminal else None\n",
    "           }\n",
    "\n",
    "def isterminal(node):\n",
    "    return node['branches'] is None\n",
    "\n",
    "def isexpanded(node):\n",
    "    return len(node['branches'] or {}) > 0\n",
    "\n",
    "def backpropagate(node, reward, rate=0.1):\n",
    "    while node is not None:\n",
    "        node['visits'] += 1\n",
    "        node['value']  += (reward - node['value']) / node['visits']\n",
    "#         node['value']  += (reward - node['value']) * rate\n",
    "        node            = node['parent']\n",
    "        reward          = -reward\n",
    "                \n",
    "def UCT(node, action, exploration): \n",
    "    n     = node['visits']\n",
    "    child = node['branches'][action]\n",
    "    nj    = child['visits']\n",
    "    xj    = child['value']\n",
    "    if exploration == 0:\n",
    "        return xj\n",
    "    if n == 0 or nj == 0:\n",
    "        return float('inf')\n",
    "    return xj + exploration*sqrt(2*log(n)/nj)\n",
    "\n",
    "def selectbranchpi(node, exploration=0):\n",
    "    assert(isexpanded(node))\n",
    "    actions  = list(node['branches'].keys())\n",
    "    values   = array([ UCT(node,a,exploration) for a in actions ])\n",
    "    probs    = argmaxprob(values)\n",
    "    return actions,probs\n",
    "\n",
    "def selectbranch(node, state, exploration):\n",
    "    actions,probs  = selectbranchpi(node, exploration)\n",
    "    action         = choice(actions, p=probs)\n",
    "    next_node      = node['branches'][action]\n",
    "    next_state,_   = game(state, action)\n",
    "    return next_node,next_state\n",
    "\n",
    "def expandbranch(model, node, state):\n",
    "    assert(not isterminal(node))\n",
    "    assert(not isexpanded(node))\n",
    "    branches = node['branches']\n",
    "    actor    = player(state)\n",
    "    aa,qq    = model(state, logits=True)\n",
    "    for a,q in zip(aa,qq):\n",
    "        _,winner    = game(state,a)\n",
    "        child       = makenode(value=q, parent=node, terminal=winner is not None)\n",
    "        branches[a] = child        \n",
    "        if isterminal(child):\n",
    "            reward = getreward(actor, winner) \n",
    "            backpropagate(child, reward) \n",
    "\n",
    "def selectnode(model, node, state, exploration):\n",
    "    while isexpanded(node):\n",
    "        node,state = selectbranch(node, state, exploration)\n",
    "    if not isterminal(node):\n",
    "        expandbranch(model, node, state)\n",
    "        node,state = selectbranch(node, state, exploration)\n",
    "    return node,state\n",
    "\n",
    "def simulatefrom(leaf, state, policy):\n",
    "    if isterminal(leaf):\n",
    "        return leaf['value']\n",
    "    actor        = player(state)\n",
    "    winner,_,_,_ = episode(policy, start=state)\n",
    "    reward       = getreward(actor, winner)\n",
    "    return reward \n",
    "        \n",
    "def search(model, node, state, policy, exploration=1):\n",
    "    leaf,state = selectnode(model, node, state, exploration)\n",
    "    reward     = simulatefrom(leaf, state, policy)\n",
    "    backpropagate(leaf, reward)\n",
    "    return reward\n",
    "\n",
    "last_action = None\n",
    "def selectaction(s,pi):\n",
    "    global last_action\n",
    "    aa,pp       = pi(s)\n",
    "    i           = choice(range(len(pp)), p=pp)\n",
    "    last_action = aa[i]\n",
    "    return aa[i],pp[i]\n",
    "\n",
    "def searchpi(model, \n",
    "             iters       = 1, \n",
    "             seconds     = 0, \n",
    "             exploration = 1, \n",
    "             policy      = {1:randompi,2:randompi}):\n",
    "    node = None\n",
    "    @rename(f'searchpi({model.name},{iters},{seconds}s,{policy[1].__name__}-vs-{policy[2].__name__})')\n",
    "    def searchpi(s):\n",
    "        nonlocal node\n",
    "        if node is None or all(s == 0):\n",
    "            node = makenode(value=0)\n",
    "        else:\n",
    "            node = node['branches'][last_action]\n",
    "        i,t  = iters,time()\n",
    "        while i > 0 or time() - t < seconds:\n",
    "            search(model, node, s, policy, exploration)\n",
    "            i -= 1\n",
    "        aa,pp = selectbranchpi(node, exploration=0)     \n",
    "        if exploration == 0:\n",
    "            node = None\n",
    "        return aa,pp\n",
    "    return searchpi\n",
    "\n",
    "def pushsample(model, state, action, reward):    \n",
    "    ss = rotate_and_flip(state)\n",
    "    aa = rotate_and_flip(action)\n",
    "    xx = array([onehot(s) for s in ss])\n",
    "    zz = model.predict(xx) \n",
    "    for z,a in zip(zz,aa):\n",
    "        z[int(a)] = reward\n",
    "    model.push(xx,zz)\n",
    "\n",
    "def train(model, \n",
    "          iters       = 1_000, \n",
    "          games       = 10, \n",
    "          searches    = 10, \n",
    "          exploration = 1, \n",
    "          discount    = 0.9, \n",
    "          clone_every = -1,\n",
    "          rates       = {0:0.1}):\n",
    "    agentpi  = searchpi(model, iters=searches, seconds=0, exploration=exploration)\n",
    "    print(f'searching under policy {agentpi.__name__}')\n",
    "    errors   = zeros(iters) \n",
    "    rewards  = [None, zeros(iters), zeros(iters)]\n",
    "    progbar  = Progbar(target=iters, stateful_metrics=['clone','rate'])\n",
    "    progress = PROGRESS(path=f'{DIR}/progress.log')\n",
    "    for i in range(iters):\n",
    "        if clone_every > 0: \n",
    "            if i % clone_every == 0:\n",
    "                clone   = model.clone()\n",
    "                agentpi = searchpi(clone, iters=searches, seconds=0, exploration=exploration)\n",
    "                clone   = i\n",
    "        else:\n",
    "            clone = i\n",
    "        if i in rates:\n",
    "            rate = rates[i]\n",
    "            K.set_value(model.optimizer.lr, rate)\n",
    "        ss,aa,pp,rr = samplegames(policy={1:agentpi,2:agentpi}, iters=games)\n",
    "        yy          = getoutcome(ss,aa,pp,rr,discount=discount)\n",
    "        for s,a,y in zip(ss,aa,yy):\n",
    "            pushsample(model,s,a,y)\n",
    "        for s,r in zip(ss,rr):\n",
    "            actor = player(s)\n",
    "            rewards[actor][i] += r/games         \n",
    "        xx,yy     = model.pop()\n",
    "        errors[i] = model.train_on_batch(x=xx, y=yy, reset_metrics=False)\n",
    "        values    = [\n",
    "            ('clone',  clone),\n",
    "            ('rate',   rate),\n",
    "            ('error',  errors[i]),\n",
    "            ('reward', rewards[1][i]),\n",
    "        ]        \n",
    "        progbar.update(i+1, values=values)\n",
    "        progress(f'{ipynb}, iter {i} of {iters}, {dict(values)}') \n",
    "        assert(rewards[1][i] == -rewards[2][i])\n",
    "    figure()\n",
    "    plot(rewards[1])\n",
    "    plot(rewards[2])\n",
    "    plot(errors,'r')\n",
    "    title('objective history')\n",
    "    ylabel('objective')\n",
    "    xlabel(f'games x{games}')\n",
    "    legend(['reward[X]', 'reward[O]', 'error'], loc='upper left')\n",
    "    savefig(f'{DIR}/{model.name}.png')\n",
    "    show()     \n",
    "\n",
    "seed(41)   \n",
    "          \n",
    "model = qmodel('mctsmodel',18,36,9,activation='tanh',buffer=100,batch=20)  \n",
    "print(model.summary())\n",
    "\n",
    "if model.exists():\n",
    "    model.load()\n",
    "else:\n",
    "    train(model, \n",
    "          iters       = 10_000, \n",
    "          games       = 10,\n",
    "          searches    = 10,\n",
    "          exploration = 10,\n",
    "          discount    = 0.9,s\n",
    "          clone_every = 1000,\n",
    "          rates       = {0:0.1, 1_000:0.05, 2_000:0.01, 5_000:0.005, 10_000:0.001, 20_000:0.0005, 25_000:0.0001})\n",
    "    model.save()\n",
    "mtime2str(model.path())\n",
    "\n",
    "agentpi = maxpi(model) \n",
    "\n",
    "testgames({1:agentpi, 2:lookaheadpi(randompi)}, iters=TESTS)\n",
    "testgames({2:agentpi, 1:lookaheadpi(randompi)}, iters=TESTS)\n",
    "\n",
    "enum_policies([\n",
    "    {1:agentpi, 2:randompi},\n",
    "    {1:agentpi, 2:lookaheadpi(randompi)},\n",
    "    {1:agentpi, 2:qpi(dptable)},\n",
    "    {1:agentpi, 2:qpi(mctable)},\n",
    "    {1:agentpi, 2:qpi(tdtable)},\n",
    "    {1:agentpi, 2:qpi(mctsdata)},\n",
    "    {1:agentpi, 2:maxpi(pimodel2)},\n",
    "    {2:agentpi, 1:randompi},\n",
    "    {2:agentpi, 1:lookaheadpi(randompi)},\n",
    "    {2:agentpi, 1:qpi(dptable)},\n",
    "    {2:agentpi, 1:qpi(mctable)},\n",
    "    {2:agentpi, 1:qpi(tdtable)},\n",
    "    {2:agentpi, 1:qpi(mctsdata)},\n",
    "    {2:agentpi, 1:maxpi(pimodel1)},\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agentpi = searchpi(model, iters=100, seconds=1, exploration=0, policy={1:randompi,2:randompi})\n",
    "agentpi = maxpi(model)\n",
    "play(policy={2:agentpi})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(model.buffer))\n",
    "\n",
    "xx = [ x for x,z in model.buffer ]\n",
    "xx.sort(key=count_nonzero)\n",
    "\n",
    "for i,x in enumerate(xx):\n",
    "    print(i, x, count_nonzero(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning: A bit of theory\n",
    "\n",
    "###### Definitions\n",
    "\n",
    "$$\\large q_{\\pi}(s_t, a_t) = \\sum_{t'=t}^T \\mathbb{E}_{\\pi_\\theta}[r(s_{t'}, a_{t'}) | s_t, a_t]$$\n",
    "\n",
    "$$\\large v_{\\pi}(s_t) = \\sum_{t'=t}^T \\mathbb{E}_{\\pi_\\theta}[r(s_{t'}, a_{t'}) | s_t]$$\n",
    "\n",
    "$$\\large p_{\\theta}(s_1,a_1,...,s_T,a_T) = p(s_1) \\prod_{t=1}^T \\pi_{\\theta}(a_t|s_t)p(s_{t+1}|s_t,a_t)$$\n",
    "\n",
    "###### Bellman Expectation Equations\n",
    "\n",
    "$$\\large v_{\\pi}(s) = \\mathbb{E}[q_{\\pi}(s, a)]$$\n",
    "\n",
    "$$\\large v_{\\pi}(s) = \\sum_a \\pi(a | s) q_{\\pi}(s, a)$$\n",
    "\n",
    "$$\\large q_{\\pi}(s, a) = \\sum_{s', r} p(s', r | s, a)[r + \\gamma v_{\\pi}(s')]$$\n",
    "\n",
    "$$\\large v_{\\pi}(s) = \\sum_a \\pi(a | s) \\sum_{s', r} p(s', r | s, a)[r + \\gamma v_{\\pi}(s')]$$\n",
    "\n",
    "$$\\large q_{\\pi}(s, a) = \\sum_{s', r} p(s', r | s, a)[r + \\gamma \\sum_{a'} \\pi(a' | s') q_{\\pi}(s', a')]$$\n",
    "\n",
    "###### Bellman Optimality Equations\n",
    "\n",
    "$$\\large v_{\\ast}(s) = \\max_a q_{\\ast}(s, a)$$\n",
    "\n",
    "$$\\large q_{\\ast}(s, a) = \\sum_{s', r} p(s', r | s, a)[r + \\gamma v_{\\ast}(s')]$$\n",
    "\n",
    "$$\\large v_{\\ast}(s) = \\max_a \\sum_{s', r} p(s', r | s, a)[r + \\gamma v_{\\ast}(s')]$$\n",
    "\n",
    "$$\\large q_{\\ast}(s, a) = \\sum_{s', r} p(s', r | s, a)[r + \\gamma \\max_{a'} q_{\\ast}(s', a')]$$\n",
    "\n",
    "###### Policy Improvement Theorem\n",
    "\n",
    "$$\\large q_\\pi(s, \\pi'(s)) \\geq v_\\pi(s) \\implies v_{\\pi'}(s) \\geq v_{\\pi}(s) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
