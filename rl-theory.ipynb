{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0325a2e4",
   "metadata": {},
   "source": [
    "## Reinforcement Learning: A bit of theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9e9ab6",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------\n",
    "#### Notations and terminology\n",
    "\n",
    "**Supervised learning** relies on labels of target outcomes for training models to predict these outcomes on unseen data.\n",
    "**Unsupervised learning** does not need labels; it learns inherent patterns on training data to group unseen data according to these learned patterns.\n",
    "Motivations behind *Reinforcement Learning* are indeed cases in which you cannot define a complete supervision and can only define feedback signals based on actions taken, and cases in which you’d like to learn the optimal decisions to make as time progresses.\n",
    "\n",
    "To define an **RL** framework, you need to define a goal for a learning system (the **agent**) that makes decisions in an **environment**. Then you need to translate this goal into a mathematical formula called a **reward** function, aimed at rewarding or penalizing the agent when it takes an action and acting as a feedback loop to help the agent reach the predefined goal. There are three key RL components: **state**, **action**, and **reward**; at each time step, the agent receives a representation of the environment’s state $\\large s_t $, takes an action $\\large a_t $, and receives a numerical reward $\\large r_{t+1}$.\n",
    "\n",
    "The general RL problem consists of learning entire sequences of actions, called *policies*, and noted $\\large \\pi(a|s) $, to maximize reward.\n",
    "\n",
    "We can formalize the objective of learning when the goal is to maximize the cumulative reward after a time step $\\large t $ by defining the concept of cumulative return $\\large G_t $:\n",
    "$$ \\large G_t = r_{t+1} + r_{t+2} + ... + r_{T} $$\n",
    "\n",
    "When there is a notion of a final step, each subsequence of actions is called an **episode**, and the [MDP](https://en.wikipedia.org/wiki/Markov_decision_process) is said to have a **finite horizon**. If the agent-environment interaction goes on forever, the MDP is said to have an **infinite horizon**; in which case you can introduce a concept of discounting to define a mathematically unified notation for cumulative return:\n",
    "$$ \\large G_t = \\sum_k^T \\gamma^k r_{t+1+k} $$\n",
    "\n",
    "If $\\large \\gamma = 1 $ and $\\large T $ is finite, this notation is the finite horizon formula for $\\large G_t $, while if $\\large \\gamma < 1 $, episodes naturally emerge even if $\\large T $ is infinite because terms with a large value of $\\large k $  become exponentially insignificant compared to terms with a smaller value of $\\large k $. Thus the sum converges for infinite horizon MDP, and this notation applies for both finite and infinite horizon MDP.\n",
    "\n",
    "------------------------------------------------------------------------------------\n",
    "#### Policy-based RL\n",
    "\n",
    "In a policy-based RL, the policy is defined as a parametric function of some parameters $\\large \\theta $: $ \\large \\pi_\\theta = \\pi(a|s,\\theta) $ and estimated directly by standard gradient methods. This is done by defining a performance measure, typically the expected value of $\\large G_t $ for a given policy, and applying gradient ascent to find $\\large \\theta $ that maximizes this performance measure. $ \\large \\pi_\\theta $ can be any parameterized functional form of $\\large \\pi $ as long as it is differentiable with respect to $\\large \\theta $; thus a neural network can be used to estimate it, in which case the parameters $\\large \\theta $ are the weights of the neural network and the neural network predicts entire policies based on input states.\n",
    "\n",
    "Direct policy search is relatively simple and also very limited because it attempts to estimate an entire policy directly from a set of previously experienced policies. Combining policy-based RL with value-based RL to refine the estimate of $\\large G_t $ almost always improves accuracy and convergence of RL.\n",
    "\n",
    "------------------------------------------------------------------------------------\n",
    "#### Value-based RL\n",
    "\n",
    "An MDP consists in a sequence $ (s_0, a_0, r_1, s_1, a_1, r_2, …, s_n) $, or more generally $ (s_t, a_t, r_{t+1}, s_{t+1})_n $, and the dynamics of an MDP is fully characterized by a transition probability $ T(s,a,s’) = P(s’|s,a) $ that defines the probability for any state $\\large s $ to going to any other state $\\large s’ $, and a reward function $ r(s,a) = P(r|s,a) $ that defines the expected reward for any given state $\\large s $. The goal of RL is to learn the best policies to maximize $\\large G_t $.\n",
    "\n",
    "To evaluate a policy, define a measure for any state s called a state **value function** $\\large V(s) $ that estimates how good it is to be in s for a particular way of acting, that is, for a particular policy π:\n",
    "\n",
    "$$\\large v_{\\pi}(s) = \\mathbb{E_\\pi}(G_t|s)$$\n",
    "$$\\large v_{\\pi}(s) = \\mathbb{E_\\pi}(\\sum_k^T \\gamma^k r_{t+1+k}|s)$$\n",
    "$$\\large v_{\\pi}(s) = \\mathbb{E_\\pi}(r_{t+1} + G_{t+1}|s)$$\n",
    "\n",
    "Replacing the expectation formula by a sum over all probabilities yields the **Bellman equation**:\n",
    "\n",
    "$$\\large v_{\\pi}(s) = \\sum_a \\pi(a | s) \\sum_{s'} T(s,a,s’)[r(s,a) + \\gamma v_{\\pi}(s')]$$\n",
    "\n",
    "You can estimate $ T $ and $ r $ by counting all the occurrences of observed transitions and rewards in a set of observed interactions between the agent and the environment. $ T $ and $ r $ define the model of the MDP.\n",
    "\n",
    "We can extend the notion of state value function to the notion of **state-action value function** $\\large Q(s,a) $ to define an optimal policy:\n",
    "\n",
    "$$\\large Q_{\\pi}(s,a) = \\sum_{s'} T(s,a,s’)[r(s,a) + \\gamma Q_{\\pi}(s',a')]$$\n",
    "\n",
    "$$\\large Q^{\\ast}_{\\pi}(s,a) = \\sum_{s'} T(s,a,s’)[r(s,a) + \\gamma \\max_{a'} Q^{\\ast}_{\\pi}(s',a')]$$\n",
    "\n",
    "The latter is called the **Bellman Optimality equation**; it estimates the value of taking a particular action $\\large a $ in a particular state $\\large s $ assuming you’ll take the best actions thereafter. If you transform the Bellman Optimality equation into an assignment function you get an iterative algorithm that, assuming you know $\\large T $ and $\\large r $, is guaranteed to converge to the optimal policy with random sampling of all possible actions in all states. Because you can apply this equation to states in any order, this iterative algorithm is referred to as **asynchronous dynamic programming**.\n",
    "\n",
    "For systems in which you can easily compute or sample $\\large T $ and $\\large r $, this approach is sufficient and is referred to as **model-based** RL because you know the transition dynamics. But for most real-case problems, it is more convenient to produce stochastic estimates of the Q-values based on experience accumulated so far, so the RL agent learns in real time at every step and ultimately converges toward true estimates of the Q-values. To this end, combine asynchronous dynamic programming with the concept of moving average:\n",
    "$$\\large Q^{\\ast}_{k+1}(s,a) = Q^{\\ast}_{k}(s,a) + \\alpha(G_t(s) - Q^{\\ast}_{k}(s,a)) $$\n",
    "\n",
    "and replace $\\large G_t(s) $ by a sampled value as defined in the Bellman Optimality equation:\n",
    "$$\\large Q^{\\ast}_{k+1}(s,a) = Q^{\\ast}_{k}(s,a) + \\alpha(r_{t+1} + \\gamma \\max_{a'} Q^{\\ast}_k(s',a') - Q^{\\ast}_{k}(s,a)) $$\n",
    "\n",
    "The latter is called **temporal difference learning**; it enables you to update the moving average $\\large Q^{\\ast}_{k+1}(s,a) $ based on the difference between $ Q^{\\ast}_{k}(s,a) $ and $ Q^{\\ast}_{k}(s',a') $. This post shows a one-step temporal difference, as used in the most straightforward version of the **Q-learning algorithm**, but you could extend this equation to include more than one step (n-step temporal difference). Again, some theorems exist that prove Q-learning converges to the optimal policy $\\large \\pi^{\\ast} $ assuming infinite random action selection.\n",
    "\n",
    "------------------------------------------------------------------------------------\n",
    "#### RL action-selection strategy\n",
    "\n",
    "You can alleviate the infinite random action selection condition by using a more efficient random action selection strategy such as **ε-Greedy** to increase sampling of states frequently encountered in good policies and decrease sampling of less valuable states. In ε-Greedy, the agent selects a random action with probability $\\large ε $, and the rest of the time (that is, with probability $\\large 1-ε $), the agent selects the best action according to the latest Q-values, which is defined as $ argmax_{a} Q(s,a) $. Generally, $\\large ε $ is chosen to be large at the beginning to favor exploration of state-action space, and progressively reduced to a smaller value.\n",
    "\n",
    "------------------------------------------------------------------------------------\n",
    "#### Combining deep learning and reinforcement learning\n",
    "\n",
    "In many cases, for example when playing chess or Go, the number of states, actions, and combinations thereof is so large that the memory and time needed to store the array of Q-values is enormous. There are more combinations of states and actions in the game Go than known stars in the universe. Instead of storing Q-values for all states and actions in an array which is impossible for Go and many other cases, deep RL attempts to generalize experience from a subset of states and actions to new states and actions. \n",
    "\n",
    "Deep Q-learning uses a supervised learning approximation to $ Q(s,a) $ by using $ r_{t+1} + \\gamma \\max_{a'} Q^{\\ast}_k(s',a') $ as the label, because the Q-learning assignment function is equivalent to a gradient update of the general form $ x = x – ∝∇J $(for any arbitrary x) where:\n",
    "$$\\large J = \\dfrac{1}{2} (Q^{\\ast}_{k}(s,a) - (r_{t+1} + \\gamma \\max_{a'} Q^{\\ast}_k(s',a')))^2 $$ \n",
    "The latter is called the **Square Bellman Loss**; it allows you to estimate Q-values based on generalization from mapping states and actions to Q-values using a deep learning network, hence the name deep Q-learning.\n",
    "\n",
    "Deep Q-learning is not guaranteed to converge anymore because the label used for supervised learning depends on current values of the network weights, which are themselves updated based on learning from the labels, hence a problem of broken ergodicity. But this approach is often good enough in practice and can generalize to the most complex RL problems (such as autonomous driving, robotics, or playing Go). \n",
    "\n",
    "In deep RL, the training set changes at every step, so you need to define a buffer to enable batch training, and schedule regular refreshes as experience accumulates for the agent to learn in real time. This is called **experience replay** and can be compared to the process of learning while dreaming in biological systems: experience replay allows you to keep optimizing policies over a subset of experiences $ (s_t, a_t, r_{t+1}, s_{t+1})_n $ kept in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9f1fff",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------\n",
    "#### Definitions\n",
    "\n",
    "$$\\large q_{\\pi}(s_t, a_t) = \\sum_{t'=t}^T \\mathbb{E}_{\\pi_\\theta}[r(s_{t'}, a_{t'}) | s_t, a_t]$$\n",
    "\n",
    "$$\\large v_{\\pi}(s_t) = \\sum_{t'=t}^T \\mathbb{E}_{\\pi_\\theta}[r(s_{t'}, a_{t'}) | s_t]$$\n",
    "\n",
    "$$\\large p_{\\theta}(s_1,a_1,...,s_T,a_T) = p(s_1) \\prod_{t=1}^T \\pi_{\\theta}(a_t|s_t)p(s_{t+1}|s_t,a_t)$$\n",
    "\n",
    "------------------------------------------------------------------------------------\n",
    "#### Bellman Expectation Equations\n",
    "\n",
    "$$\\large v_{\\pi}(s) = \\mathbb{E}[q_{\\pi}(s, a)]$$\n",
    "\n",
    "$$\\large v_{\\pi}(s) = \\sum_a \\pi(a | s) q_{\\pi}(s, a)$$\n",
    "\n",
    "$$\\large q_{\\pi}(s, a) = \\sum_{s', r} p(s', r | s, a)[r + \\gamma v_{\\pi}(s')]$$\n",
    "\n",
    "$$\\large v_{\\pi}(s) = \\sum_a \\pi(a | s) \\sum_{s', r} p(s', r | s, a)[r + \\gamma v_{\\pi}(s')]$$\n",
    "\n",
    "$$\\large q_{\\pi}(s, a) = \\sum_{s', r} p(s', r | s, a)[r + \\gamma \\sum_{a'} \\pi(a' | s') q_{\\pi}(s', a')]$$\n",
    "\n",
    "------------------------------------------------------------------------------------\n",
    "#### Bellman Optimality Equations\n",
    "\n",
    "$$\\large v_{\\ast}(s) = \\max_a q_{\\ast}(s, a)$$\n",
    "\n",
    "$$\\large q_{\\ast}(s, a) = \\sum_{s', r} p(s', r | s, a)[r + \\gamma v_{\\ast}(s')]$$\n",
    "\n",
    "$$\\large v_{\\ast}(s) = \\max_a \\sum_{s', r} p(s', r | s, a)[r + \\gamma v_{\\ast}(s')]$$\n",
    "\n",
    "$$\\large q_{\\ast}(s, a) = \\sum_{s', r} p(s', r | s, a)[r + \\gamma \\max_{a'} q_{\\ast}(s', a')]$$\n",
    "\n",
    "------------------------------------------------------------------------------------\n",
    "#### Policy Improvement Theorem\n",
    "\n",
    "$$\\large q_\\pi(s, \\pi'(s)) \\geq v_\\pi(s) \\implies v_{\\pi'}(s) \\geq v_{\\pi}(s) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a7f3c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba0956df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[NbConvertApp] Converting notebook rl-theory.ipynb to HTML',\n",
       " '[NbConvertApp] Writing 663061 bytes to rl-theory.html']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system(`jupyter-nbconvert --output-dir=./ --to HTML rl-theory.ipynb`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8631a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
