{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning series: Tic-tac-toe game bot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-25 16:23:58.341568: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-25 16:23:58.454736: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-03-25 16:23:58.972133: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-25 16:23:58.972186: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-25 16:23:58.972191: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIR = tmp/tic-tac-toe\n"
     ]
    }
   ],
   "source": [
    "### IMPORTS ###\n",
    "\n",
    "ipynb = 'tic-tac-toe'\n",
    "\n",
    "import sys, os, json\n",
    "import tensorflow    as tf\n",
    "import keras.backend as K\n",
    "\n",
    "from ipywidgets           import widgets, HBox, VBox, Layout\n",
    "from IPython.display      import display, HTML, Javascript as JS\n",
    "from pandas               import DataFrame\n",
    "from pathlib              import Path\n",
    "from pprint               import pprint\n",
    "from operator             import iconcat\n",
    "from functools            import reduce, partial\n",
    "from collections          import deque\n",
    "from numpy                import *\n",
    "from numpy.random         import *\n",
    "from os.path              import isfile\n",
    "from uuid                 import uuid4 as guid\n",
    "\n",
    "from tensorflow                      import keras\n",
    "from tensorflow.keras.layers         import Input, Dense, BatchNormalization, Activation, Multiply\n",
    "from tensorflow.keras.losses         import mse, categorical_crossentropy, binary_crossentropy\n",
    "from tensorflow.keras.optimizers     import Adam\n",
    "from tensorflow.keras.regularizers   import l2\n",
    "from tensorflow.keras.activations    import softmax\n",
    "from tensorflow.keras.models         import Model, load_model, clone_model\n",
    "from tensorflow.keras.callbacks      import LearningRateScheduler, LambdaCallback\n",
    "from tensorflow.keras.utils          import Progbar, to_categorical\n",
    "\n",
    "from matplotlib.pyplot    import *\n",
    "from time                 import *\n",
    "\n",
    "# %matplotlib inline\n",
    "\n",
    "DIR = f'tmp/{ipynb}'\n",
    "Path(DIR).mkdir(parents=True, exist_ok=True)\n",
    "print('DIR =', DIR)\n",
    "\n",
    "TESTS = 100\n",
    "\n",
    "def rename(newname):\n",
    "    def decorator(f):\n",
    "        f.__name__ = newname\n",
    "        return f\n",
    "    return decorator\n",
    "\n",
    "def time2str(t):\n",
    "    return strftime(\"%b %d %Y %H:%M:%S\", localtime(t))\n",
    "\n",
    "def mtime(path):\n",
    "    return os.path.getmtime(path)\n",
    "\n",
    "def mtime2str(path):\n",
    "    return time2str(mtime(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem formulation: Treating after-states as action-value function inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 506us/step - samples per episode: 6.9000 - total samples: 69.0000\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 2.]\n",
      " [0. 0. 0. 1. 0. 0. 1. 0. 2.]\n",
      " [0. 0. 0. 1. 2. 0. 1. 0. 2.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [2. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [2. 1. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [2. 1. 0. 0. 0. 2. 0. 0. 1.]\n",
      " [2. 1. 0. 0. 0. 2. 1. 0. 1.]\n",
      " [2. 1. 2. 0. 0. 2. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 2. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 2. 1. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 2. 1. 2. 0. 0. 0. 1. 0.]\n",
      " [0. 2. 1. 2. 1. 0. 0. 1. 0.]\n",
      " [0. 2. 1. 2. 1. 0. 2. 1. 0.]\n",
      " [0. 2. 1. 2. 1. 1. 2. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 2. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 2. 0. 1. 0. 1. 0. 0. 0.]\n",
      " [0. 2. 0. 1. 2. 1. 0. 0. 0.]\n",
      " [0. 2. 0. 1. 2. 1. 1. 0. 0.]\n",
      " [0. 2. 0. 1. 2. 1. 1. 0. 2.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 2. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 1. 2. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 1. 2. 2. 0. 0.]\n",
      " [1. 1. 0. 0. 1. 2. 2. 0. 0.]\n",
      " [1. 1. 2. 0. 1. 2. 2. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 2. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 2. 0. 1.]\n",
      " [2. 0. 1. 0. 0. 0. 2. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [2. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [2. 0. 0. 1. 0. 1. 0. 0. 0.]\n",
      " [2. 2. 0. 1. 0. 1. 0. 0. 0.]\n",
      " [2. 2. 0. 1. 0. 1. 0. 0. 1.]\n",
      " [2. 2. 0. 1. 2. 1. 0. 0. 1.]\n",
      " [2. 2. 0. 1. 2. 1. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 2. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 1. 2. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 1. 2. 0. 0. 2.]\n",
      " [0. 1. 0. 0. 1. 2. 1. 0. 2.]\n",
      " [0. 1. 0. 0. 1. 2. 1. 2. 2.]\n",
      " [0. 1. 0. 1. 1. 2. 1. 2. 2.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [2. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [2. 0. 1. 0. 0. 0. 0. 0. 1.]\n",
      " [2. 2. 1. 0. 0. 0. 0. 0. 1.]\n",
      " [2. 2. 1. 0. 1. 0. 0. 0. 1.]\n",
      " [2. 2. 1. 0. 1. 2. 0. 0. 1.]\n",
      " [2. 2. 1. 1. 1. 2. 0. 0. 1.]\n",
      " [2. 2. 1. 1. 1. 2. 0. 2. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 2. 0. 0. 0.]\n",
      " [1. 0. 1. 0. 0. 2. 0. 0. 0.]\n",
      " [1. 0. 1. 0. 0. 2. 0. 2. 0.]]\n",
      "[3. 8. 6. 4. 0. 1. 0. 8. 5. 6. 2. 7. 7. 1. 2. 3. 4. 6. 5. 0. 5. 1. 3. 4.\n",
      " 6. 8. 0. 4. 5. 0. 6. 1. 2. 8. 8. 6. 2. 0. 5. 3. 0. 5. 1. 8. 4. 6. 2. 4.\n",
      " 5. 1. 8. 6. 7. 3. 2. 8. 0. 2. 1. 4. 5. 3. 7. 6. 2. 5. 0. 7. 1.]\n",
      "[0.11111111 0.125      0.14285714 0.16666667 0.2        0.11111111\n",
      " 0.125      0.14285714 0.16666667 0.2        0.25       0.33333333\n",
      " 0.11111111 0.125      0.14285714 0.16666667 0.2        0.25\n",
      " 0.33333333 0.5        0.11111111 0.125      0.14285714 0.16666667\n",
      " 0.2        0.25       0.33333333 0.11111111 0.125      0.14285714\n",
      " 0.16666667 0.2        0.25       0.33333333 0.11111111 0.125\n",
      " 0.14285714 0.16666667 0.2        0.11111111 0.125      0.14285714\n",
      " 0.16666667 0.2        0.25       0.33333333 0.5        0.11111111\n",
      " 0.125      0.14285714 0.16666667 0.2        0.25       0.33333333\n",
      " 0.5        0.11111111 0.125      0.14285714 0.16666667 0.2\n",
      " 0.25       0.33333333 0.5        1.         0.11111111 0.125\n",
      " 0.14285714 0.16666667 0.2       ]\n",
      "[ 0.  0.  0. -1.  1.  0.  0.  0.  0.  0. -1.  1.  0.  0.  0.  0.  0.  0.\n",
      " -1.  1.  0.  0.  0.  0.  0. -1.  1.  0.  0.  0.  0.  0. -1.  1.  0.  0.\n",
      "  0. -1.  1.  0.  0.  0.  0.  0.  0. -1.  1.  0.  0.  0.  0.  0.  0. -1.\n",
      "  1.  0.  0.  0.  0.  0.  0.  0. -1.  1.  0.  0.  0. -1.  1.]\n",
      "testing policy 1:randompi vs 2:randompi\n",
      "100/100 [==============================] - 0s 451us/step - draws: 0.0011 - wins1: 0.0056 - wins2: 0.0033 - win ratio: 1.5974          \n",
      "enumerating policy 1:randompi vs 2:randompi with EnumProc\n",
      "enumerating policy 1:lookaheadpi(randompi) vs 2:randompi with EnumProc\n",
      "enumerating policy 1:randompi vs 2:lookaheadpi(randompi) with EnumProc\n",
      "enumerating policy 1:lookaheadpi(randompi) vs 2:lookaheadpi(randompi) with EnumProc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy1</th>\n",
       "      <th>policy2</th>\n",
       "      <th>states1</th>\n",
       "      <th>states2</th>\n",
       "      <th>draws</th>\n",
       "      <th>wins1</th>\n",
       "      <th>wins2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>randompi</td>\n",
       "      <td>randompi</td>\n",
       "      <td>2739</td>\n",
       "      <td>2739</td>\n",
       "      <td>16</td>\n",
       "      <td>626</td>\n",
       "      <td>316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lookaheadpi(randompi)</td>\n",
       "      <td>randompi</td>\n",
       "      <td>2701</td>\n",
       "      <td>2581</td>\n",
       "      <td>16</td>\n",
       "      <td>604</td>\n",
       "      <td>316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>randompi</td>\n",
       "      <td>lookaheadpi(randompi)</td>\n",
       "      <td>2701</td>\n",
       "      <td>2733</td>\n",
       "      <td>16</td>\n",
       "      <td>620</td>\n",
       "      <td>316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lookaheadpi(randompi)</td>\n",
       "      <td>lookaheadpi(randompi)</td>\n",
       "      <td>2657</td>\n",
       "      <td>2569</td>\n",
       "      <td>16</td>\n",
       "      <td>604</td>\n",
       "      <td>316</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 policy1                policy2  states1  states2  draws  \\\n",
       "0               randompi               randompi     2739     2739     16   \n",
       "1  lookaheadpi(randompi)               randompi     2701     2581     16   \n",
       "2               randompi  lookaheadpi(randompi)     2701     2733     16   \n",
       "3  lookaheadpi(randompi)  lookaheadpi(randompi)     2657     2569     16   \n",
       "\n",
       "   wins1  wins2  \n",
       "0    626    316  \n",
       "1    604    316  \n",
       "2    620    316  \n",
       "3    604    316  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### ENVIRONMENT ###\n",
    "\n",
    "# 0 1 2\n",
    "# 3 4 5\n",
    "# 6 7 8\n",
    "\n",
    "winidx = [\n",
    "    [ [0,1,2],          [0,3,6], [0,4,8] ], # 0 - (0,0)\n",
    "    [ [0,1,2],          [1,4,7]          ], # 1 - (0,1)\n",
    "    [ [0,1,2], [2,4,6], [2,5,8]          ], # 2 - (0,2)\n",
    "    [ [3,4,5],          [0,3,6]          ], # 3 - (1,0)\n",
    "    [ [3,4,5], [2,4,6], [1,4,7], [0,4,8] ], # 4 - (1,1)\n",
    "    [ [3,4,5],          [2,5,8]          ], # 5 - (1,2)\n",
    "    [ [6,7,8], [2,4,6], [0,3,6],         ], # 6 - (2,0)\n",
    "    [ [6,7,8],          [1,4,7]          ], # 7 - (2,1)\n",
    "    [ [6,7,8],          [2,5,8], [0,4,8] ], # 8 - (2,2)\n",
    "]\n",
    "\n",
    "encoding = array([\n",
    "    [0,0], # 0\n",
    "    [0,1], # 1\n",
    "    [1,0], # 2\n",
    "]) \n",
    "\n",
    "def onehot(s): \n",
    "    return concatenate(encoding[s.astype(int)])\n",
    "\n",
    "def digits(s):\n",
    "    return ''.join(int64(s).ravel().astype(str))\n",
    "\n",
    "def other(p): \n",
    "    return (p%2)+1\n",
    "\n",
    "def player(s):\n",
    "    return other(count_nonzero(s))\n",
    "\n",
    "def action2xy(a):\n",
    "    x,y = unravel_index(a, (3,3))\n",
    "    return (x,y)\n",
    "\n",
    "def xy2action(x,y):\n",
    "    a = ravel_multi_index((x,y), (3,3))\n",
    "    return a\n",
    "\n",
    "def enum_lines(a):\n",
    "    return winidx[a]\n",
    "            \n",
    "def iswin(s,a):\n",
    "    p = s[a]\n",
    "    for line in enum_lines(a):\n",
    "        if all(s[line] == p):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def canwin(s,a,p):\n",
    "    a    = int(a)\n",
    "    s[a] = p\n",
    "    yes  = iswin(s,a)\n",
    "    s[a] = 0\n",
    "    return yes\n",
    "\n",
    "def hasbegun(s):\n",
    "    return count_nonzero(s) > 0\n",
    "\n",
    "def isover(s):\n",
    "    return count_nonzero(s) == len(s)\n",
    "\n",
    "def game(s=None,a=None):\n",
    "    winner = None\n",
    "    if s is None:\n",
    "        s = zeros(3*3)\n",
    "    elif a is None:\n",
    "        s = array(list(s)).astype(float)\n",
    "    else:\n",
    "        a    = int(a)\n",
    "        assert(s[a] == 0)\n",
    "        s    = copy(s)\n",
    "        p    = player(s)\n",
    "        s[a] = p\n",
    "        if iswin(s,a):\n",
    "            winner = p\n",
    "        elif isover(s):\n",
    "            winner = 0\n",
    "    return s,winner    \n",
    "\n",
    "def actions(s):\n",
    "    aa = argwhere(s == 0)\n",
    "    return concatenate(aa) if len(aa) > 0 else []\n",
    "\n",
    "def actionmask(aa):\n",
    "    aa       = aa.astype(int)\n",
    "    mask     = zeros(9)\n",
    "    mask[aa] = 1\n",
    "    return mask\n",
    "\n",
    "def selectaction(s,pi):\n",
    "    aa,pp = pi(s)\n",
    "    i     = choice(range(len(pp)), p=pp)\n",
    "    return aa[i],pp[i]    \n",
    "\n",
    "def getreward(agent, winner):\n",
    "    rival = other(agent)\n",
    "    if winner == agent:\n",
    "        return 1\n",
    "    if winner == rival:\n",
    "        return -1\n",
    "    return 0\n",
    "\n",
    "def getoutcome(ss,aa,pp,rr,discount):\n",
    "    outcome = {1:0,2:0}\n",
    "    n       = len(rr)\n",
    "    yy      = zeros_like(rr)\n",
    "    for i in reversed(range(n)):\n",
    "        actor          = player(ss[i])\n",
    "        _,winner       = game(ss[i],aa[i])      \n",
    "        if winner is not None:\n",
    "            outcome    = {1:0,2:0}\n",
    "        yy[i]          = rr[i] + outcome[actor]\n",
    "        outcome[actor] = yy[i] * discount\n",
    "    return yy\n",
    "\n",
    "def episode(policy, start=None):\n",
    "    n  = 0\n",
    "    ss = zeros((9,9))\n",
    "    aa = zeros((9))\n",
    "    pp = zeros((9))\n",
    "    s,winner = game() if start is None else (start,None)\n",
    "    while winner is None:\n",
    "        actor   = player(s)\n",
    "        rival   = other(actor) \n",
    "        a,p     = selectaction(s, policy[actor])\n",
    "        ss[n,:] = s\n",
    "        aa[n]   = a\n",
    "        pp[n]   = p\n",
    "        n      += 1     \n",
    "        s,winner = game(s, a)\n",
    "    ss = ss[0:n,:]\n",
    "    aa = aa[0:n]\n",
    "    pp = pp[0:n]\n",
    "    return winner,ss,aa,pp\n",
    "\n",
    "def winratio(wins, agent=1):\n",
    "    rival       = other(agent)\n",
    "    draw_score  = wins[0] * 0.5\n",
    "    agent_score = wins[agent] + draw_score\n",
    "    rival_score = wins[rival] + draw_score\n",
    "    ratio       = agent_score / (rival_score or 1)\n",
    "    return ratio\n",
    "\n",
    "def testgames(policy, iters=1000):\n",
    "    ratio    = 0\n",
    "    agent,*_ = policy.keys()\n",
    "    rival    = other(agent)\n",
    "    wins     = [0,0,0]\n",
    "    progbar  = Progbar(target=iters, stateful_metrics=['draws','wins1','wins2','win ratio'])\n",
    "    print(f'testing policy {agent}:{policy[agent].__name__} vs {rival}:{policy[rival].__name__}')\n",
    "    for i in range(iters):\n",
    "        winner,_,_,_  = episode(policy)\n",
    "        wins[winner] += 1\n",
    "        ratio         = winratio(wins,agent)\n",
    "        progbar.update(i+1, values=[\n",
    "            ('draws', wins[0]/10_000),\n",
    "            ('wins1', wins[1]/10_000),\n",
    "            ('wins2', wins[2]/10_000),\n",
    "            ('win ratio', ratio),             \n",
    "        ])    \n",
    "    return ratio\n",
    "\n",
    "def samplegames(policy, iters=100, start=None, progress=None):\n",
    "    m       = 0\n",
    "    sss     = zeros((9*iters,9))\n",
    "    aaa     = zeros((9*iters))\n",
    "    ppp     = zeros((9*iters))\n",
    "    rrr     = zeros((9*iters))\n",
    "    progbar = Progbar(target=iters, stateful_metrics=['total samples']) if progress else None\n",
    "    for i in range(iters):\n",
    "        winner,ss,aa,pp = episode(policy, start=start)\n",
    "        rr              = zeros((len(ss)))\n",
    "        rr[-1]          = getreward(player(ss[-1]), winner)\n",
    "        rr[-2]          = getreward(player(ss[-2]), winner)\n",
    "        d               = len(rr)\n",
    "        sss[m:m+d,:]    = ss\n",
    "        aaa[m:m+d]      = aa\n",
    "        ppp[m:m+d]      = pp\n",
    "        rrr[m:m+d]      = rr\n",
    "        m              += d\n",
    "        if progbar is not None:\n",
    "            progbar.update(i+1, values=[\n",
    "                ('samples per episode', d),\n",
    "                ('total samples', m),\n",
    "            ])    \n",
    "    return sss[0:m,:], aaa[0:m], ppp[0:m], rrr[0:m]\n",
    "\n",
    "class EnumProc:\n",
    "    def __init__(self):\n",
    "        self.__name__ = 'EnumProc'\n",
    "        self.states   = [0,0,0]\n",
    "        self.wins     = [0,0,0]\n",
    "    def __call__(self, s, winner):\n",
    "        p = player(s)\n",
    "        self.states[p] += 1\n",
    "        if winner is not None:\n",
    "            self.wins[winner] += 1\n",
    "    \n",
    "def enum_policy(policy, proc):\n",
    "    print(f'enumerating policy 1:{policy[1].__name__} vs 2:{policy[2].__name__} with {proc.__name__}')\n",
    "    visited = {}\n",
    "    def iter(s,w):\n",
    "        key = digits(s)\n",
    "        if key not in visited:\n",
    "            visited[key] = 0\n",
    "            proc(s,w)\n",
    "            if w is None:\n",
    "                actor = player(s)\n",
    "                aa,pp = policy[actor](s)\n",
    "                for a,p in zip(aa,pp):\n",
    "                    if p > 0:\n",
    "                        iter(*game(s,a))\n",
    "        visited[key] += 1\n",
    "    iter(*game())\n",
    "\n",
    "def enum_policies(policies):\n",
    "    policy1,policy2,states1,states2,draws,wins1,wins2 = [],[],[],[],[],[],[]\n",
    "    for policy in policies:\n",
    "        proc = EnumProc()\n",
    "        enum_policy(policy, proc)\n",
    "        policy1.append(policy[1].__name__)\n",
    "        policy2.append(policy[2].__name__)\n",
    "        states1.append(proc.states[1])\n",
    "        states2.append(proc.states[2])\n",
    "        draws.append(proc.wins[0])\n",
    "        wins1.append(proc.wins[1])\n",
    "        wins2.append(proc.wins[2])\n",
    "    return DataFrame(data={\n",
    "        'policy1': policy1,\n",
    "        'policy2': policy2,\n",
    "        'states1': states1,\n",
    "        'states2': states2,\n",
    "        'draws': draws,\n",
    "        'wins1': wins1,\n",
    "        'wins2': wins2\n",
    "    })    \n",
    "    \n",
    "def argsmax(values):\n",
    "    return unique(ravel(argwhere(values == max(values))))\n",
    "        \n",
    "def lookahead(s, aa):\n",
    "    actor = player(s)\n",
    "    ww    = [game(s,a)[1] for a in aa]\n",
    "    ii    = [i for i,w in enumerate(ww) if w == actor]\n",
    "#     if len(ii) == 0:\n",
    "#         rival = other(actor)\n",
    "#         ii    = [i for i,a in enumerate(aa) if canwin(s,a,rival)]\n",
    "    return ii\n",
    "    \n",
    "def uniformprob(aa,ii):\n",
    "    pp     = zeros_like(aa).astype(float)\n",
    "    pp[ii] = array(1/len(ii))\n",
    "    return pp\n",
    "\n",
    "def argmaxprob(qq):\n",
    "    ii = argsmax(qq)\n",
    "    pp = uniformprob(qq,ii)\n",
    "    return ravel(pp)\n",
    "\n",
    "def softmaxprob(zz):\n",
    "    max_z = max(zz)\n",
    "    num   = exp(zz - max_z) \n",
    "    den   = sum(num)\n",
    "    return num / den\n",
    "\n",
    "def randompi(s):\n",
    "    aa = actions(s)\n",
    "    n  = len(aa)\n",
    "    pp = array([1/n] * n)\n",
    "    return aa,pp\n",
    "\n",
    "def lookaheadpi(pi):\n",
    "    @rename(f'lookaheadpi({pi.__name__})')\n",
    "    def lookaheadpi(s):\n",
    "        aa = actions(s)\n",
    "        ii = lookahead(s, aa)\n",
    "        if len(ii) > 0: \n",
    "            pp = uniformprob(aa,ii)\n",
    "            return aa,pp\n",
    "        return pi(s)    \n",
    "    return lookaheadpi\n",
    "\n",
    "def maxpi(pi):\n",
    "    @rename(f'maxpi({pi.__name__})')\n",
    "    def maxpi(s):\n",
    "        aa,pp = pi(s)\n",
    "        pp    = argmaxprob(pp)\n",
    "        return aa,pp\n",
    "    return maxpi\n",
    "\n",
    "def percentilepi(pi, centile=90):\n",
    "    @rename(f'percentilepi({pi.__name__})')\n",
    "    def percentilepi(s):\n",
    "        aa,pp = pi(s)\n",
    "        th    = percentile(pp,centile)\n",
    "        ii    = argwhere(pp >= th)\n",
    "        pp    = uniformprob(aa,ii)\n",
    "        return aa,pp\n",
    "    return percentilepi\n",
    "\n",
    "def explorepi(pi, epsilon=0.1):\n",
    "    @rename(f'explorepi({pi.__name__},{epsilon})')\n",
    "    def explorepi(s):\n",
    "        if random() < epsilon:\n",
    "            return randompi(s)\n",
    "        return pi(s)\n",
    "    return explorepi\n",
    "\n",
    "def choicepi(*policies):\n",
    "    names = ','.join([pi.__name__ for pi in policies])\n",
    "    @rename(f'choicepi({names})')\n",
    "    def choicepi(s):\n",
    "        pi = choice(policies)\n",
    "        return pi(s)\n",
    "    return choicepi\n",
    "\n",
    "def switchpi(*policies):\n",
    "    pi    = choice(policies)\n",
    "    names = ','.join([pi.__name__ for pi in policies])\n",
    "    @rename(f'switchpi({names})')\n",
    "    def switchpi(s):\n",
    "        nonlocal pi\n",
    "        if count_nonzero(s) < 2:\n",
    "            pi = choice(policies)\n",
    "        return pi(s)\n",
    "    return switchpi\n",
    "\n",
    "seed(42)\n",
    "\n",
    "ss,aa,pp,rr = samplegames({ 1:randompi, 2:randompi }, iters=10, progress=True)\n",
    "print(ss)\n",
    "print(aa)\n",
    "print(pp)\n",
    "print(rr)\n",
    "\n",
    "testgames({ 1:randompi, 2:randompi },iters=TESTS)\n",
    "\n",
    "enum_policies([\n",
    "    {1:randompi,              2:randompi},\n",
    "    {1:lookaheadpi(randompi), 2:randompi},\n",
    "    {1:randompi,              2:lookaheadpi(randompi)},\n",
    "    {1:lookaheadpi(randompi), 2:lookaheadpi(randompi)},\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter Widgets: Simple game frontend to try agents right here in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "play against lookaheadpi(randompi)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c901c00908fe4c40bf40d942690c756e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Button(layout=Layout(height='40px', width='40px'), style=ButtonStyle(), tooltip=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### FRONTEND ###\n",
    "\n",
    "def play(policy):\n",
    "    state   = None\n",
    "    board   = None\n",
    "    agent   = None\n",
    "    rival   = None\n",
    "    \n",
    "    def moveagent(s, a):\n",
    "        aa,pp    = policy[agent](s)\n",
    "        i        = choice(range(len(pp)), p=pp)\n",
    "        a,p      = aa[i],pp[i]\n",
    "        s,winner = game(s, a)\n",
    "        return s,winner\n",
    "\n",
    "    def display_board(onclick):\n",
    "        board = []\n",
    "        for i in range(9):\n",
    "            btn = widgets.Button(\n",
    "                description  = '',\n",
    "                disabled     = False,\n",
    "                button_style = '', # 'success', 'info', 'warning', 'danger' or ''\n",
    "                tooltip      = 'Click me',\n",
    "                icon         = '',\n",
    "                layout       = Layout(width='40px', height='40px')\n",
    "            )\n",
    "            btn.action = i\n",
    "            btn.on_click(lambda btn: onclick(btn.action))\n",
    "            board.append(btn)\n",
    "        display(VBox([\n",
    "            HBox([board[0],board[1],board[2]]),\n",
    "            HBox([board[3],board[4],board[5]]),\n",
    "            HBox([board[6],board[7],board[8]])\n",
    "        ]))\n",
    "        return board\n",
    "        \n",
    "    def update_board(board, state):\n",
    "        chars = [' ', 'x', 'o']\n",
    "        state = state.astype(int)\n",
    "        for i in range(9):\n",
    "            board[i].description = chars[state[i]]\n",
    "\n",
    "    def gameturn(s=None, a=None):\n",
    "        s,winner = game(s,a)\n",
    "#         print(reshape(s, (3,3)))\n",
    "        update_board(board, s)\n",
    "        if winner is None and player(s) == agent:\n",
    "            s,winner = moveagent(s,a)\n",
    "            update_board(board, s)\n",
    "        if winner is not None:\n",
    "            msgs = ['DRAW','X WINS','O WINS']\n",
    "            print(msgs[winner])\n",
    "            for i in range(9):\n",
    "                board[i].disabled = True\n",
    "            play(policy)\n",
    "        return s,winner\n",
    "\n",
    "    def onclick(a): \n",
    "        nonlocal state\n",
    "        if (player(state) != agent) and (a in actions(state)):\n",
    "            state,winner = gameturn(state,a)\n",
    "\n",
    "    assert(1 in policy or 2 in policy)    \n",
    "    if 1 in policy:\n",
    "        agent,rival = 1,2\n",
    "    elif 2 in policy:\n",
    "        agent,rival = 2,1\n",
    "    print(f'play against {policy[agent].__name__}')\n",
    "    \n",
    "    board        = display_board(onclick=onclick)\n",
    "    state,winner = gameturn()\n",
    "\n",
    "play(policy={1:lookaheadpi(randompi)})\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabular Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Programming: Action-value iteration\n",
    "Action-value iteration works by iteratively applying the Bellman optimality equation for $q_{\\ast}$ to a working action-value function, as an update rule, as shown below.\n",
    "\n",
    "$$\\large q_{\\ast}(s, a) = \\sum_{s', r} p(s', r | s, a)[r + \\gamma v_{\\ast}(s')]$$\n",
    "\n",
    "Alternatively we can express this equation in terms of $q_{\\ast}$ itself.\n",
    "\n",
    "$$\\large q_{\\ast}(s, a) = \\sum_{s', r} p(s', r | s, a)[r + \\gamma \\max_{a'} q_{\\ast}(s', a')]$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dptable: 5478\n",
      "testing policy 1:qpi(dptable) vs 2:lookaheadpi(randompi)\n",
      "100/100 [==============================] - 0s 1ms/step - draws: 0.0000e+00 - wins1: 0.0100 - wins2: 0.0000e+00 - win ratio: 100.0000\n",
      "testing policy 2:qpi(dptable) vs 1:lookaheadpi(randompi)\n",
      "100/100 [==============================] - 0s 1ms/step - draws: 9.0000e-04 - wins1: 0.0000e+00 - wins2: 0.0091 - win ratio: 21.2222\n",
      "testing policy 1:qpi(dptable) vs 2:qpi(dptable)\n",
      "100/100 [==============================] - 0s 2ms/step - draws: 0.0100 - wins1: 0.0000e+00 - wins2: 0.0000e+00 - win ratio: 1.0000\n",
      "enumerating policy 1:qpi(dptable) vs 2:lookaheadpi(randompi) with EnumProc\n",
      "enumerating policy 1:lookaheadpi(randompi) vs 2:qpi(dptable) with EnumProc\n",
      "enumerating policy 1:qpi(dptable) vs 2:qpi(dptable) with EnumProc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy1</th>\n",
       "      <th>policy2</th>\n",
       "      <th>states1</th>\n",
       "      <th>states2</th>\n",
       "      <th>draws</th>\n",
       "      <th>wins1</th>\n",
       "      <th>wins2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>qpi(dptable)</td>\n",
       "      <td>lookaheadpi(randompi)</td>\n",
       "      <td>1219</td>\n",
       "      <td>1004</td>\n",
       "      <td>7</td>\n",
       "      <td>554</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lookaheadpi(randompi)</td>\n",
       "      <td>qpi(dptable)</td>\n",
       "      <td>577</td>\n",
       "      <td>737</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>qpi(dptable)</td>\n",
       "      <td>qpi(dptable)</td>\n",
       "      <td>25</td>\n",
       "      <td>26</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 policy1                policy2  states1  states2  draws  \\\n",
       "0           qpi(dptable)  lookaheadpi(randompi)     1219     1004      7   \n",
       "1  lookaheadpi(randompi)           qpi(dptable)      577      737     16   \n",
       "2           qpi(dptable)           qpi(dptable)       25       26      7   \n",
       "\n",
       "   wins1  wins2  \n",
       "0    554      0  \n",
       "1      0    260  \n",
       "2      0      0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TABULAR-DYNAMIC-PROGRAMMING ###\n",
    "\n",
    "class qtable(dict):\n",
    "    def __init__(self, name):\n",
    "        self.__name__ = name\n",
    "    def __call__(self, s, a):\n",
    "        s,_ = game(s,a)\n",
    "        key = digits(s)\n",
    "        q   = self.get(key,0)\n",
    "        return q\n",
    "    def path(self):\n",
    "        return f'{DIR}/{self.__name__}.npy'\n",
    "    def save(self):\n",
    "        save(self.path(), self)\n",
    "    def load(self):\n",
    "        tmp = load(self.path(), allow_pickle=True)\n",
    "        self.update(tmp.item())\n",
    "    def exists(self):\n",
    "        return isfile(self.path())\n",
    "    \n",
    "def qpi(model):\n",
    "    @rename(f'qpi({model.__name__})')\n",
    "    def qpi(s):\n",
    "        aa = actions(s)\n",
    "        qq = array([model(s,a) for a in aa])\n",
    "        pp = argmaxprob(qq)\n",
    "        return aa,pp\n",
    "    return qpi\n",
    "\n",
    "def enum_states(proc):\n",
    "    visited = {}\n",
    "    def iter(s,w):\n",
    "        key = digits(s)\n",
    "        if key not in visited:\n",
    "            visited[key] = 1\n",
    "            proc(s,w)\n",
    "            if w is None:\n",
    "                for a in actions(s):\n",
    "                    iter(*game(s,a))\n",
    "    iter(*game())\n",
    "\n",
    "def iter_qvalue(table, iters=10, target=1e-8):    \n",
    "    def eval_afterstate(after_state, winner):    \n",
    "        new_value = 0\n",
    "        rival     = player(after_state)\n",
    "        agent     = other(rival)\n",
    "        if winner is None: \n",
    "            rival_actions = actions(after_state)\n",
    "            rival_prob    = 1 / len(rival_actions)\n",
    "            for rival_action in rival_actions:\n",
    "                next_state,winner = game(after_state, rival_action)\n",
    "                if winner is None: \n",
    "                    max_next_value = max([table(next_state,a) for a in actions(next_state)])\n",
    "                    new_value     += rival_prob * (0 + max_next_value)\n",
    "                else:\n",
    "                    assert(winner != agent)\n",
    "                    reward     = -1 if winner == rival else 0\n",
    "                    new_value += rival_prob * (reward + 0)\n",
    "                    if winner == rival:              # commenting this  \n",
    "                        new_value = 1 * (reward + 0) # prevents player2\n",
    "                        break                        # from learning optimal policy\n",
    "        else:\n",
    "            assert(winner != rival)\n",
    "            reward    = 1 if winner == agent else 0\n",
    "            new_value = 1 * (reward + 0)\n",
    "        key        = digits(after_state)\n",
    "        old_value  = table.get(key, 0)\n",
    "        table[key] = new_value\n",
    "        return abs(old_value - new_value)\n",
    "    delta = 0\n",
    "    def proc(state, winner):\n",
    "        nonlocal delta\n",
    "        delta = max(delta, eval_afterstate(state, winner))\n",
    "    progbar = Progbar(target=iters, stateful_metrics=['delta'])\n",
    "    for i in range(iters):\n",
    "        delta = 0\n",
    "        enum_states(proc)\n",
    "        progbar.update(i+1, values=[('delta',  delta)]) \n",
    "        if delta <= target:\n",
    "            break\n",
    "    print('\\n')\n",
    "         \n",
    "dptable = qtable('dptable')  \n",
    "if dptable.exists():\n",
    "    dptable.load()\n",
    "else:\n",
    "    iter_qvalue(dptable, iters=100, target=1e-08)\n",
    "    dptable.save()\n",
    "print('dptable:', len(dptable))\n",
    "mtime2str(dptable.path())\n",
    "\n",
    "agentpi = qpi(dptable) \n",
    "rivalpi = lookaheadpi(randompi)\n",
    "\n",
    "testgames({1:agentpi, 2:rivalpi}, iters=TESTS)\n",
    "testgames({2:agentpi, 1:rivalpi}, iters=TESTS)\n",
    "testgames({1:agentpi, 2:agentpi}, iters=TESTS)\n",
    "\n",
    "enum_policies([\n",
    "    {1:agentpi, 2:rivalpi},\n",
    "    {1:rivalpi, 2:agentpi},\n",
    "    {1:agentpi, 2:agentpi},\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play(policy={2:agentpi})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte-Carlo RL:\n",
    "\n",
    "$$\\large V(S_t) = V(S_t) + \\alpha [ G_t - V(S_t) ]$$\n",
    "\n",
    "$$\\large Q(S_t,A_t) = Q(S_t,A_t) + \\alpha [ G_t - Q(S_t,A_t) ]$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mctable: 4732\n",
      "testing policy 1:qpi(mctable) vs 2:lookaheadpi(randompi)\n",
      "100/100 [==============================] - 0s 1ms/step - draws: 0.0000e+00 - wins1: 0.0100 - wins2: 0.0000e+00 - win ratio: 100.0000\n",
      "testing policy 2:qpi(mctable) vs 1:lookaheadpi(randompi)\n",
      "100/100 [==============================] - 0s 1ms/step - draws: 0.0012 - wins1: 0.0000e+00 - wins2: 0.0088 - win ratio: 15.6667 \n",
      "testing policy 1:qpi(mctable) vs 2:qpi(mctable)\n",
      "100/100 [==============================] - 0s 2ms/step - draws: 0.0100 - wins1: 0.0000e+00 - wins2: 0.0000e+00 - win ratio: 1.0000\n",
      "enumerating policy 1:qpi(mctable) vs 2:lookaheadpi(randompi) with EnumProc\n",
      "enumerating policy 1:lookaheadpi(randompi) vs 2:qpi(mctable) with EnumProc\n",
      "enumerating policy 1:qpi(mctable) vs 2:qpi(mctable) with EnumProc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy1</th>\n",
       "      <th>policy2</th>\n",
       "      <th>states1</th>\n",
       "      <th>states2</th>\n",
       "      <th>draws</th>\n",
       "      <th>wins1</th>\n",
       "      <th>wins2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>qpi(mctable)</td>\n",
       "      <td>lookaheadpi(randompi)</td>\n",
       "      <td>94</td>\n",
       "      <td>87</td>\n",
       "      <td>3</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lookaheadpi(randompi)</td>\n",
       "      <td>qpi(mctable)</td>\n",
       "      <td>264</td>\n",
       "      <td>318</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>qpi(mctable)</td>\n",
       "      <td>qpi(mctable)</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 policy1                policy2  states1  states2  draws  \\\n",
       "0           qpi(mctable)  lookaheadpi(randompi)       94       87      3   \n",
       "1  lookaheadpi(randompi)           qpi(mctable)      264      318     12   \n",
       "2           qpi(mctable)           qpi(mctable)        5        5      1   \n",
       "\n",
       "   wins1  wins2  \n",
       "0     63      0  \n",
       "1      0    148  \n",
       "2      0      0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TABULAR-MONTE-CARLO ###\n",
    "\n",
    "def iter_under_policy(policy, iters=100, games=1, rates={0:0.1}, epsilons={0:0.1}, proc=print):\n",
    "    print(f'iterating under policy {1}:{policy[1].__name__} vs {2}:{policy[2].__name__}')\n",
    "    progbar  = Progbar(target=iters, stateful_metrics=['rate', 'epsilon'])\n",
    "    errors   = zeros(iters) \n",
    "    rewards  = [None, zeros(iters), zeros(iters)]\n",
    "    for i in range(iters):\n",
    "        if i in rates:\n",
    "            rate = rates[i]\n",
    "        if i in epsilons:\n",
    "            epsilon = epsilons[i]\n",
    "            policy2 = {\n",
    "                1:explorepi(policy[1], epsilon),\n",
    "                2:explorepi(policy[2], epsilon),\n",
    "            }\n",
    "        ss,aa,pp,rr = samplegames(policy=policy2, iters=games)\n",
    "        errors[i]   = proc(ss,aa,pp,rr,rate=rate) or 0\n",
    "        for s,r in zip(ss,rr):\n",
    "            actor = player(s)\n",
    "            rewards[actor][i] += r/games \n",
    "        values = [\n",
    "            ('rate',      rate),\n",
    "            ('epsilon',   epsilon), \n",
    "            ('error',     errors[i]),\n",
    "            ('reward[X]', rewards[1][i]),\n",
    "            ('reward[O]', rewards[2][i])\n",
    "        ]\n",
    "        progbar.update(i+1, values)\n",
    "#         progress(f'{ipynb}, iter {i} of {iters}, {dict(values)}')    \n",
    "    figure()\n",
    "    plot(errors,'r')\n",
    "    plot(rewards[1])\n",
    "    plot(rewards[2])\n",
    "    title('objective history')\n",
    "    ylabel('objective')\n",
    "    xlabel(f'games x{games}')\n",
    "    legend(['error', 'reward[X]', 'reward[O]'], loc='upper left')\n",
    "    savefig(f'{DIR}/{policy[1].__name__}-vs-{policy[2].__name__}.png')\n",
    "    show()     \n",
    "\n",
    "def mctrain(table, discount=0.99, iters=100, games=1, rates={0:0.1}, epsilons={0:0.1}):\n",
    "    def mcproc(ss,aa,pp,rr,rate=0.1):\n",
    "        error = 0\n",
    "        yy    = getoutcome(ss,aa,pp,rr,discount=discount)\n",
    "        for s,a,y in zip(ss,aa,yy):\n",
    "            key         = digits(game(s,a)[0])\n",
    "            q           = table.get(key, 0)\n",
    "            diff        = y - q\n",
    "            table[key]  = q + rate*diff\n",
    "            error      += abs(diff)\n",
    "        return error / len(ss)\n",
    "    agentpi = lookaheadpi(qpi(table))\n",
    "    policy  = {1:agentpi,2:agentpi}\n",
    "    iter_under_policy(policy, iters=iters, games=games, rates=rates, epsilons=epsilons, proc=mcproc)\n",
    "        \n",
    "seed(42)\n",
    "mctable = qtable('mctable')   \n",
    "if mctable.exists():\n",
    "    mctable.load()\n",
    "else:\n",
    "    mctrain(mctable, \n",
    "            discount = 0.99, \n",
    "            iters    = 10_000, \n",
    "            games    = 10, \n",
    "            rates    = { 0:0.1 }, \n",
    "            epsilons = { 0:0.1, 5000:0.05 })\n",
    "    mctable.save()\n",
    "print('mctable:', len(mctable))\n",
    "mtime2str(mctable.path())\n",
    "          \n",
    "agentpi = qpi(mctable) \n",
    "rivalpi = lookaheadpi(randompi)\n",
    "\n",
    "testgames({1:agentpi, 2:rivalpi}, iters=TESTS)\n",
    "testgames({2:agentpi, 1:rivalpi}, iters=TESTS)\n",
    "testgames({1:agentpi, 2:agentpi}, iters=TESTS)\n",
    "\n",
    "enum_policies([\n",
    "    {1:agentpi, 2:rivalpi},\n",
    "    {1:rivalpi, 2:agentpi},\n",
    "    {1:agentpi, 2:agentpi},\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play(policy={2:agentpi})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal difference: Q-Learning\n",
    "\n",
    "$$\\large V(S_t) = V(S_t) + \\alpha [ R_{t+1} + \\gamma V(S_{t+1}) - V(S_t) ]$$\n",
    "\n",
    "$$\\large Q(S_t, A_t) = Q(S_t, A_t) + \\alpha [ R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) ]$$\n",
    "\n",
    "$$\\large Q(S_t, A_t) = Q(S_t, A_t) + \\alpha [ R_{t+1} + \\gamma \\max_{a} Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) ]$$\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tdtable: 4461\n",
      "testing policy 1:qpi(tdtable) vs 2:lookaheadpi(randompi)\n",
      "100/100 [==============================] - 0s 1ms/step - draws: 0.0000e+00 - wins1: 0.0100 - wins2: 0.0000e+00 - win ratio: 100.0000\n",
      "testing policy 2:qpi(tdtable) vs 1:lookaheadpi(randompi)\n",
      "100/100 [==============================] - 0s 1ms/step - draws: 1.0000e-03 - wins1: 0.0000e+00 - wins2: 0.0090 - win ratio: 19.0000\n",
      "testing policy 1:qpi(tdtable) vs 2:qpi(tdtable)\n",
      "100/100 [==============================] - 0s 2ms/step - draws: 0.0100 - wins1: 0.0000e+00 - wins2: 0.0000e+00 - win ratio: 1.0000\n",
      "enumerating policy 1:qpi(tdtable) vs 2:lookaheadpi(randompi) with EnumProc\n",
      "enumerating policy 1:lookaheadpi(randompi) vs 2:qpi(tdtable) with EnumProc\n",
      "enumerating policy 1:qpi(tdtable) vs 2:qpi(tdtable) with EnumProc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy1</th>\n",
       "      <th>policy2</th>\n",
       "      <th>states1</th>\n",
       "      <th>states2</th>\n",
       "      <th>draws</th>\n",
       "      <th>wins1</th>\n",
       "      <th>wins2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>qpi(tdtable)</td>\n",
       "      <td>lookaheadpi(randompi)</td>\n",
       "      <td>90</td>\n",
       "      <td>87</td>\n",
       "      <td>4</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lookaheadpi(randompi)</td>\n",
       "      <td>qpi(tdtable)</td>\n",
       "      <td>260</td>\n",
       "      <td>312</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>qpi(tdtable)</td>\n",
       "      <td>qpi(tdtable)</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 policy1                policy2  states1  states2  draws  \\\n",
       "0           qpi(tdtable)  lookaheadpi(randompi)       90       87      4   \n",
       "1  lookaheadpi(randompi)           qpi(tdtable)      260      312     12   \n",
       "2           qpi(tdtable)           qpi(tdtable)        5        5      1   \n",
       "\n",
       "   wins1  wins2  \n",
       "0     62      0  \n",
       "1      0    146  \n",
       "2      0      0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TABULAR-TEMPORAL-DIFFERENCE ###\n",
    "        \n",
    "def tdtrain(table, discount=0.99, iters=100, games=1, rates={0:0.1}, epsilons={0:0.1}):\n",
    "    def tdproc(ss,aa,pp,rr,rate=0.1):\n",
    "        error = 0\n",
    "        n     = len(ss)\n",
    "        for j in reversed(range(n)):\n",
    "            reward    = rr[j]\n",
    "            after_s,_ = game(ss[j],aa[j])\n",
    "            key       = digits(after_s)\n",
    "            q         = table.get(key, 0)\n",
    "            if j+1 < n and all(after_s == ss[j+1]):\n",
    "                next_s,_   = game(ss[j+1],aa[j+1])\n",
    "                qq         = [table(next_s,a) for a in actions(next_s)]\n",
    "                max_next_q = max(qq) if len(qq) > 0 else 0\n",
    "            else:    \n",
    "                max_next_q = 0\n",
    "            diff       = reward + discount*max_next_q - q\n",
    "            table[key] = q + rate*diff                \n",
    "            error     += abs(diff)\n",
    "        return error / n\n",
    "    agentpi = lookaheadpi(qpi(table))\n",
    "    policy  = {1:agentpi,2:agentpi}\n",
    "    iter_under_policy(policy, iters=iters, games=games, rates=rates, epsilons=epsilons, proc=tdproc)\n",
    "        \n",
    "seed(42)\n",
    "tdtable = qtable('tdtable')   \n",
    "if tdtable.exists():\n",
    "    tdtable.load()\n",
    "else:\n",
    "    tdtrain(tdtable, \n",
    "        discount = 0.99, \n",
    "        iters    = 10_000, \n",
    "        games    = 30, \n",
    "        rates    = {0:0.1, 3_000:0.01, 7_000:0.001}, \n",
    "        epsilons = {0:0.1, 5_000: 0.05})\n",
    "    tdtable.save()\n",
    "print('tdtable:', len(tdtable))\n",
    "mtime2str(tdtable.path())\n",
    "      \n",
    "agentpi = qpi(tdtable) \n",
    "rivalpi = lookaheadpi(randompi)\n",
    "\n",
    "testgames({1:agentpi, 2:rivalpi}, iters=TESTS)\n",
    "testgames({2:agentpi, 1:rivalpi}, iters=TESTS)\n",
    "testgames({1:agentpi, 2:agentpi}, iters=TESTS)\n",
    "\n",
    "enum_policies([\n",
    "    {1:agentpi, 2:rivalpi},\n",
    "    {1:rivalpi, 2:agentpi},\n",
    "    {1:agentpi, 2:agentpi},\n",
    "])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play(policy={2:agentpi})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Tree Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\large UCT(S,a) = \\frac{Q(S_a)}{N(S_a)} + \\alpha*\\sqrt{\\frac{2\\ln{N(S)}}{N(S_a)}}$$\n",
    "\n",
    "$$\\large PUCT(S,a) = \\frac{Q(S_a)}{N(S_a)} + \\alpha*P(S,a)\\sqrt{\\frac{\\sum_b{N(S_b)}}{(1+N(S_a))}}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rot0 [0 1 2 3 4 5 6 7 8] [0 1 2 3 4 5 6 7 8]\n",
      "rot90 [2 5 8 1 4 7 0 3 6] [6 3 0 7 4 1 8 5 2]\n",
      "rot180 [8 7 6 5 4 3 2 1 0] [8 7 6 5 4 3 2 1 0]\n",
      "rot270 [6 3 0 7 4 1 8 5 2] [2 5 8 1 4 7 0 3 6]\n",
      "fliplr [2 1 0 5 4 3 8 7 6] [2 1 0 5 4 3 8 7 6]\n",
      "flipud [6 7 8 3 4 5 0 1 2] [6 7 8 3 4 5 0 1 2]\n",
      "fliptlbr [8 5 2 7 4 1 6 3 0] [8 5 2 7 4 1 6 3 0]\n",
      "fliptrbl [0 3 6 1 4 7 2 5 8] [0 3 6 1 4 7 2 5 8]\n",
      "\n",
      "testing policy 1:searchpi(6239,0.1) vs 2:lookaheadpi(randompi)\n",
      "10/10 [==============================] - 3s 313ms/step - draws: 0.0000e+00 - wins1: 1.0000e-03 - wins2: 0.0000e+00 - win ratio: 10.0000\n",
      "testing policy 2:searchpi(6239,0.1) vs 1:lookaheadpi(randompi)\n",
      "10/10 [==============================] - 4s 370ms/step - draws: 3.0000e-04 - wins1: 0.0000e+00 - wins2: 7.0000e-04 - win ratio: 5.6667\n",
      "enumerating policy 1:qpi(mctsdata) vs 2:randompi with EnumProc\n",
      "enumerating policy 1:qpi(mctsdata) vs 2:lookaheadpi(randompi) with EnumProc\n",
      "enumerating policy 1:qpi(mctsdata) vs 2:qpi(dptable) with EnumProc\n",
      "enumerating policy 1:qpi(mctsdata) vs 2:qpi(mctable) with EnumProc\n",
      "enumerating policy 1:qpi(mctsdata) vs 2:qpi(tdtable) with EnumProc\n",
      "enumerating policy 1:randompi vs 2:qpi(mctsdata) with EnumProc\n",
      "enumerating policy 1:lookaheadpi(randompi) vs 2:qpi(mctsdata) with EnumProc\n",
      "enumerating policy 1:qpi(dptable) vs 2:qpi(mctsdata) with EnumProc\n",
      "enumerating policy 1:qpi(mctable) vs 2:qpi(mctsdata) with EnumProc\n",
      "enumerating policy 1:qpi(tdtable) vs 2:qpi(mctsdata) with EnumProc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy1</th>\n",
       "      <th>policy2</th>\n",
       "      <th>states1</th>\n",
       "      <th>states2</th>\n",
       "      <th>draws</th>\n",
       "      <th>wins1</th>\n",
       "      <th>wins2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>qpi(mctsdata)</td>\n",
       "      <td>randompi</td>\n",
       "      <td>177</td>\n",
       "      <td>181</td>\n",
       "      <td>8</td>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>qpi(mctsdata)</td>\n",
       "      <td>lookaheadpi(randompi)</td>\n",
       "      <td>177</td>\n",
       "      <td>181</td>\n",
       "      <td>8</td>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>qpi(mctsdata)</td>\n",
       "      <td>qpi(dptable)</td>\n",
       "      <td>29</td>\n",
       "      <td>33</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>qpi(mctsdata)</td>\n",
       "      <td>qpi(mctable)</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>qpi(mctsdata)</td>\n",
       "      <td>qpi(tdtable)</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>randompi</td>\n",
       "      <td>qpi(mctsdata)</td>\n",
       "      <td>449</td>\n",
       "      <td>589</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>lookaheadpi(randompi)</td>\n",
       "      <td>qpi(mctsdata)</td>\n",
       "      <td>449</td>\n",
       "      <td>589</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>qpi(dptable)</td>\n",
       "      <td>qpi(mctsdata)</td>\n",
       "      <td>25</td>\n",
       "      <td>26</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>qpi(mctable)</td>\n",
       "      <td>qpi(mctsdata)</td>\n",
       "      <td>17</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>qpi(tdtable)</td>\n",
       "      <td>qpi(mctsdata)</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 policy1                policy2  states1  states2  draws  \\\n",
       "0          qpi(mctsdata)               randompi      177      181      8   \n",
       "1          qpi(mctsdata)  lookaheadpi(randompi)      177      181      8   \n",
       "2          qpi(mctsdata)           qpi(dptable)       29       33      8   \n",
       "3          qpi(mctsdata)           qpi(mctable)        8        9      2   \n",
       "4          qpi(mctsdata)           qpi(tdtable)        8        9      2   \n",
       "5               randompi          qpi(mctsdata)      449      589     16   \n",
       "6  lookaheadpi(randompi)          qpi(mctsdata)      449      589     16   \n",
       "7           qpi(dptable)          qpi(mctsdata)       25       26      7   \n",
       "8           qpi(mctable)          qpi(mctsdata)       17       16      3   \n",
       "9           qpi(tdtable)          qpi(mctsdata)       17       17      4   \n",
       "\n",
       "   wins1  wins2  \n",
       "0    132      0  \n",
       "1    132      0  \n",
       "2      0      0  \n",
       "3      0      0  \n",
       "4      0      0  \n",
       "5      0    228  \n",
       "6      0    228  \n",
       "7      0      0  \n",
       "8      0      0  \n",
       "9      0      0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### MONTE-CARLO-TREE-SEARCH ###\n",
    "\n",
    "class mctstable(qtable):\n",
    "    def __init__(self, name):\n",
    "        self.__name__ = name\n",
    "        self.notfound = None\n",
    "    def __call__(self, s, a):\n",
    "        s,_  = game(s,a)\n",
    "        key  = digits(s)\n",
    "        if key in self:\n",
    "            uid = self[key]\n",
    "            return self[uid]['value']\n",
    "        if self.notfound is not None:\n",
    "            self.notfound.add(key)\n",
    "        return 0   \n",
    "    \n",
    "def rot0(M):     return M\n",
    "def rot180(M):   return rot90(M,2)\n",
    "def rot270(M):   return rot90(M,3)\n",
    "def fliptrbl(M): return transpose(M)\n",
    "def fliptlbr(M): return transpose(rot90(M,2))\n",
    "\n",
    "rotations = [ rot0,   rot90,  rot180,   rot270   ]\n",
    "flips     = [ fliplr, flipud, fliptlbr, fliptrbl ]\n",
    "\n",
    "def map_rotations_and_flips():\n",
    "    statemap  = {}\n",
    "    actionmap = {}\n",
    "    state   = array(arange(9))\n",
    "    for f in rotations+flips:\n",
    "        s1 = ravel(f(reshape(state,(3,3))))\n",
    "        s2 = zeros_like(s1)\n",
    "        for i,x in enumerate(s1):\n",
    "            s2[x] = i;\n",
    "        statemap [f.__name__] = s1\n",
    "        actionmap[f.__name__] = s2\n",
    "    return statemap,actionmap\n",
    "    \n",
    "statemap,actionmap = map_rotations_and_flips()\n",
    "for f in rotations+flips:\n",
    "    print(f.__name__, statemap[f.__name__], actionmap[f.__name__])\n",
    "print()\n",
    "    \n",
    "def rotate_and_flip(x):\n",
    "    if isscalar(x): \n",
    "        x = int(x)\n",
    "        return array([ idx[x] for idx in actionmap.values() ])\n",
    "    else: \n",
    "        return array([ x[idx] for idx in statemap.values()  ])\n",
    "    \n",
    "def getstats(table, state):\n",
    "    key = digits(state)    \n",
    "    if key not in table:\n",
    "        uid        = str(guid())\n",
    "        table[uid] = { 'value': 0, 'visits': 0 }\n",
    "        for s in rotate_and_flip(state):\n",
    "            k = digits(s)\n",
    "            if k not in table:\n",
    "                table[k] = uid\n",
    "    uid = table[key]\n",
    "    return table[uid]\n",
    "\n",
    "def makenode(table, state, parent=None, terminal=False):\n",
    "    return {'parent'   : parent,\n",
    "            'stats'    : getstats(table, state),\n",
    "            'branches' : { a:None for a in actions(state) } if not terminal else None\n",
    "           }\n",
    "\n",
    "def isterminal(node):\n",
    "    return node['branches'] is None\n",
    "\n",
    "def isexpanded(node):\n",
    "    if isterminal(node):\n",
    "        return False\n",
    "    for child in node['branches'].values():\n",
    "        if child is None:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def backpropagate(node, reward):\n",
    "    while node is not None:\n",
    "        stats            = node['stats']\n",
    "        stats['visits'] += 1\n",
    "        stats['value']  += (reward - stats['value']) / stats['visits']\n",
    "        node             = node['parent']\n",
    "        reward           = -reward\n",
    "\n",
    "def expandbranch(table, node, state):\n",
    "    assert(not isterminal(node))\n",
    "    assert(not isexpanded(node))\n",
    "    branches          = node['branches']\n",
    "    actor             = player(state)\n",
    "    actions           = list(branches.keys())\n",
    "    unknown           = [ a for a in actions if branches[a] is None ]\n",
    "    action            = choice(unknown)\n",
    "    next_state,winner = game(state,action)\n",
    "    child             = makenode(table, next_state, parent=node, terminal=winner is not None)\n",
    "    branches[action]  = child\n",
    "    if isterminal(child):\n",
    "        reward = getreward(actor, winner) \n",
    "        backpropagate(child, reward)\n",
    "    return child,next_state\n",
    "                \n",
    "def ucb1(node, action, exploration): \n",
    "    n     = node['stats']['visits']\n",
    "    child = node['branches'][action]\n",
    "    nj    = child['stats']['visits']\n",
    "    xj    = child['stats']['value']\n",
    "    if exploration == 0:\n",
    "        return xj\n",
    "    if n == 0 or nj == 0:\n",
    "        return float('inf')\n",
    "    return xj + exploration*sqrt(2*log(n)/nj)\n",
    "\n",
    "def selectbranchpi(node, exploration=0):\n",
    "    assert(isexpanded(node))\n",
    "    actions  = list(node['branches'].keys())\n",
    "    values   = array([ ucb1(node,a,exploration) for a in actions ])\n",
    "    probs    = argmaxprob(values)\n",
    "    return actions,probs\n",
    "\n",
    "def selectbranch(node, state, exploration):\n",
    "    actions,probs = selectbranchpi(node, exploration)\n",
    "    action        = choice(actions, p=probs)\n",
    "    node          = node['branches'][action]\n",
    "    state,_       = game(state, action)\n",
    "    return node,state\n",
    "\n",
    "def selectnode(table, node, state, exploration):\n",
    "    while isexpanded(node):\n",
    "        node,state = selectbranch(node, state, exploration)\n",
    "    if not isterminal(node):\n",
    "        node,state = expandbranch(table, node, state)\n",
    "    return node,state\n",
    "\n",
    "def simulatefrom(leaf, state, policy):\n",
    "    if isterminal(leaf):\n",
    "        return leaf['stats']['value']\n",
    "    actor        = player(state)\n",
    "    winner,_,_,_ = episode(policy, start=state)\n",
    "    reward       = getreward(actor, winner)\n",
    "    return reward \n",
    "    \n",
    "def search(table, node, state, policy, exploration=1):\n",
    "    leaf,state = selectnode(table, node, state, exploration)\n",
    "    reward     = simulatefrom(leaf, state, policy)\n",
    "    backpropagate(leaf, reward)\n",
    "    return reward\n",
    "\n",
    "def searchpi(table, timeout=1, policy={1:randompi,2:randompi}):\n",
    "    @rename(f'searchpi({len(table)},{timeout})')\n",
    "    def searchpi(s):\n",
    "        t    = time()\n",
    "        node = makenode(table, s)\n",
    "        search(table, node, s, policy)\n",
    "        while time() - t < timeout:\n",
    "            search(table, node, s, policy)\n",
    "        aa,pp = selectbranchpi(node, exploration=0)\n",
    "        return aa,pp\n",
    "    return searchpi\n",
    "\n",
    "def train(table, policy, iters, exploration=1):\n",
    "    state    = game()[0]\n",
    "    root     = makenode(table, state)\n",
    "    progbar  = Progbar(target=iters, stateful_metrics=[])\n",
    "#     progress = PROGRESS(path=f'{DIR}/progress.log')\n",
    "    for i in range(progbar.target):\n",
    "        reward = search(table, root, state, policy, exploration)\n",
    "        size   = len(table) \n",
    "        progbar.update(i+1, values=[('reward',reward),('size',size)])\n",
    "#         progress(f'{ipynb}, iter {i+1} of {iters}, reward {reward}, size {size}')   \n",
    "\n",
    "seed(41)   \n",
    "          \n",
    "playoutpi = switchpi(randompi, lookaheadpi(randompi)) \n",
    "          \n",
    "mctsdata = mctstable('mctsdata')      \n",
    "policy   = {1:playoutpi,2:playoutpi}\n",
    "if mctsdata.exists():\n",
    "    mctsdata.load()\n",
    "else:\n",
    "    train(mctsdata, policy, iters=500_000, exploration=10)\n",
    "    mctsdata.save()\n",
    "mtime2str(mctsdata.path())\n",
    "          \n",
    "agentpi = searchpi(mctsdata,0.1)\n",
    "rivalpi = lookaheadpi(randompi)        \n",
    "ratio1  = testgames({1:agentpi, 2:rivalpi}, iters=10)          \n",
    "ratio2  = testgames({2:agentpi, 1:rivalpi}, iters=10)          \n",
    "\n",
    "mctsdata.notfound = set() \n",
    "agentpi = qpi(mctsdata)\n",
    "enum_policies([\n",
    "    {1:agentpi, 2:randompi},\n",
    "    {1:agentpi, 2:lookaheadpi(randompi)},\n",
    "    {1:agentpi, 2:qpi(dptable)},\n",
    "    {1:agentpi, 2:qpi(mctable)},\n",
    "    {1:agentpi, 2:qpi(tdtable)},\n",
    "    {2:agentpi, 1:randompi},\n",
    "    {2:agentpi, 1:lookaheadpi(randompi)},\n",
    "    {2:agentpi, 1:qpi(dptable)},\n",
    "    {2:agentpi, 1:qpi(mctable)},\n",
    "    {2:agentpi, 1:qpi(tdtable)},\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(mctsdata.notfound))\n",
    "pprint(mctsdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play(policy={2:searchpi(mctsdata,1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximate Reinforcement Learning:  Neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Gradients: REINFORCE\n",
    "\n",
    "$$\\large \\nabla J(\\theta) \\propto \\sum_s \\mu(s) \\sum_a q_\\pi(s,a) \\nabla \\pi(a|s, \\theta)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"pimodel1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_12 (InputLayer)          [(None, 18)]         0           []                               \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 32)           608         ['input_12[0][0]']               \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 9)            297         ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " input_11 (InputLayer)          [(None, 9)]          0           []                               \n",
      "                                                                                                  \n",
      " tf.ones_like_4 (TFOpLambda)    (None, 9)            0           ['dense_11[0][0]']               \n",
      "                                                                                                  \n",
      " tf.cast_5 (TFOpLambda)         (None, 9)            0           ['input_11[0][0]']               \n",
      "                                                                                                  \n",
      " tf.math.multiply_4 (TFOpLambda  (None, 9)           0           ['tf.ones_like_4[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.where_5 (TFOpLambda)        (None, 9)            0           ['tf.cast_5[0][0]',              \n",
      "                                                                  'dense_11[0][0]',               \n",
      "                                                                  'tf.math.multiply_4[0][0]']     \n",
      "                                                                                                  \n",
      " tf.nn.softmax_5 (TFOpLambda)   (None, 9)            0           ['tf.where_5[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 905\n",
      "Trainable params: 905\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "iterating under policy 1:pimodel1 vs 2:lookaheadpi(randompi)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 's' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 70\u001b[0m\n\u001b[1;32m     68\u001b[0m     pimodel1\u001b[38;5;241m.\u001b[39mload()\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 70\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mpimodel1\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdiscount\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.99\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43miters\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgames\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrates\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepsilons\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     pimodel1\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m     77\u001b[0m mtime2str(pimodel1\u001b[38;5;241m.\u001b[39mpath())\n",
      "Cell \u001b[0;32mIn[14], line 61\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(models, discount, iters, games, rates, epsilons)\u001b[0m\n\u001b[1;32m     59\u001b[0m     error    \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtrain_on_batch(x\u001b[38;5;241m=\u001b[39m[xx,mask], y\u001b[38;5;241m=\u001b[39mohaa, sample_weight\u001b[38;5;241m=\u001b[39myy, reset_metrics\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m error\n\u001b[0;32m---> 61\u001b[0m \u001b[43miter_under_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilons\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepsilons\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 17\u001b[0m, in \u001b[0;36miter_under_policy\u001b[0;34m(policy, iters, games, rates, epsilons, proc)\u001b[0m\n\u001b[1;32m     12\u001b[0m     epsilon \u001b[38;5;241m=\u001b[39m epsilons[i]\n\u001b[1;32m     13\u001b[0m     policy2 \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;241m1\u001b[39m:explorepi(policy[\u001b[38;5;241m1\u001b[39m], epsilon),\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;241m2\u001b[39m:explorepi(policy[\u001b[38;5;241m2\u001b[39m], epsilon),\n\u001b[1;32m     16\u001b[0m     }\n\u001b[0;32m---> 17\u001b[0m ss,aa,pp,rr \u001b[38;5;241m=\u001b[39m \u001b[43msamplegames\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgames\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m errors[i]   \u001b[38;5;241m=\u001b[39m proc(ss,aa,pp,rr,rate\u001b[38;5;241m=\u001b[39mrate) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s,r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(ss,rr):\n",
      "Cell \u001b[0;32mIn[2], line 177\u001b[0m, in \u001b[0;36msamplegames\u001b[0;34m(policy, iters, start, progress)\u001b[0m\n\u001b[1;32m    175\u001b[0m progbar \u001b[38;5;241m=\u001b[39m Progbar(target\u001b[38;5;241m=\u001b[39miters, stateful_metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal samples\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m progress \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(iters):\n\u001b[0;32m--> 177\u001b[0m     winner,ss,aa,pp \u001b[38;5;241m=\u001b[39m \u001b[43mepisode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m     rr              \u001b[38;5;241m=\u001b[39m zeros((\u001b[38;5;28mlen\u001b[39m(ss)))\n\u001b[1;32m    179\u001b[0m     rr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]          \u001b[38;5;241m=\u001b[39m getreward(player(ss[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]), winner)\n",
      "Cell \u001b[0;32mIn[2], line 131\u001b[0m, in \u001b[0;36mepisode\u001b[0;34m(policy, start)\u001b[0m\n\u001b[1;32m    129\u001b[0m actor   \u001b[38;5;241m=\u001b[39m player(s)\n\u001b[1;32m    130\u001b[0m rival   \u001b[38;5;241m=\u001b[39m other(actor) \n\u001b[0;32m--> 131\u001b[0m a,p     \u001b[38;5;241m=\u001b[39m \u001b[43mselectaction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m[\u001b[49m\u001b[43mactor\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m ss[n,:] \u001b[38;5;241m=\u001b[39m s\n\u001b[1;32m    133\u001b[0m aa[n]   \u001b[38;5;241m=\u001b[39m a\n",
      "Cell \u001b[0;32mIn[2], line 97\u001b[0m, in \u001b[0;36mselectaction\u001b[0;34m(s, pi)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselectaction\u001b[39m(s,pi):\n\u001b[0;32m---> 97\u001b[0m     aa,pp \u001b[38;5;241m=\u001b[39m \u001b[43mpi\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m     i     \u001b[38;5;241m=\u001b[39m choice(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(pp)), p\u001b[38;5;241m=\u001b[39mpp)\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m aa[i],pp[i]\n",
      "Cell \u001b[0;32mIn[2], line 312\u001b[0m, in \u001b[0;36mexplorepi.<locals>.explorepi\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m random() \u001b[38;5;241m<\u001b[39m epsilon:\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m randompi(s)\n\u001b[0;32m--> 312\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpi\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 25\u001b[0m, in \u001b[0;36mpimodel.__call__\u001b[0;34m(self, input, training)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#         s,   = input\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m         aa   \u001b[38;5;241m=\u001b[39m actions(\u001b[43ms\u001b[49m)        \n\u001b[1;32m     26\u001b[0m         xx   \u001b[38;5;241m=\u001b[39m array([onehot(s)]) \n\u001b[1;32m     27\u001b[0m         mask \u001b[38;5;241m=\u001b[39m array([actionmask(aa)])\n",
      "\u001b[0;31mNameError\u001b[0m: name 's' is not defined"
     ]
    }
   ],
   "source": [
    "### POLICY-GRADIENTS ###\n",
    "\n",
    "class pimodel(Model):\n",
    "    def __init__(self, name, *nn):\n",
    "        inf  = tf.constant(np.finfo(np.float32).min)\n",
    "        mask = Input((nn[-1],)) \n",
    "        def maskedsoftmax(x):\n",
    "            ones     = tf.ones_like(x)\n",
    "            boolmask = tf.cast(mask, dtype=tf.dtypes.bool)\n",
    "            masked_x = tf.where(boolmask, x, ones*inf)\n",
    "            return softmax(masked_x)\n",
    "        x     = Input((nn[0],))\n",
    "        y     = x\n",
    "        for n in nn[1:-1]:\n",
    "            y = Dense(n, activation='relu', trainable=True)(y)\n",
    "        y     = Dense(nn[-1], trainable=True)(y)\n",
    "#         z     = Activation(maskedsoftmax)(y)\n",
    "        z     = maskedsoftmax(y)\n",
    "        super(pimodel, self).__init__(inputs=[x,mask], outputs=z, name=name)\n",
    "        optimizer = Adam(lr=0.001, clipnorm=1.0)\n",
    "        self.compile(loss=categorical_crossentropy, optimizer=optimizer)     \n",
    "        self.__name__ = name      \n",
    "    def __call__(self, s, training=None):\n",
    "#         s,   = input\n",
    "        aa   = actions(s)        \n",
    "        xx   = array([onehot(s)]) \n",
    "        mask = array([actionmask(aa)])\n",
    "        zz   = self.predict([xx,mask])\n",
    "        pp   = ravel(zz)[aa]\n",
    "        return aa,pp\n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "    def path(self):\n",
    "        return f'{DIR}/{self.__name__}.npy'\n",
    "    def save(self):\n",
    "        save(self.path(), self.get_weights())\n",
    "    def load(self):\n",
    "        weights = load(self.path())\n",
    "        self.set_weights(weights)\n",
    "    def exists(self):\n",
    "        return isfile(self.path())\n",
    "\n",
    "def train(models, discount=0.99, iters=100, games=1, rates={0:0.1}, epsilons={0:0.1}):\n",
    "    avg_y         = 0\n",
    "    agent,*_      = models.keys()\n",
    "    rival         = other(agent)\n",
    "    model         = models[agent]\n",
    "    policy        = {}    \n",
    "    policy[agent] = model\n",
    "    policy[rival] = models[rival] if rival in models else lookaheadpi(randompi)\n",
    "    def proc(ss,aa,pp,rr,rate=0.1):\n",
    "        K.set_value(model.optimizer.lr, rate)\n",
    "        yy       = getoutcome(ss,aa,pp,rr,discount=discount)\n",
    "        ii       = [i for i,s in enumerate(ss) if agent == player(s)] \n",
    "        ss,aa,yy = ss[ii],aa[ii],yy[ii]\n",
    "        xx       = array([onehot(s) for s in ss])\n",
    "        mask     = array([actionmask(actions(s)) for s in ss])\n",
    "        ohaa     = to_categorical(aa,9)\n",
    "        error    = model.train_on_batch(x=[xx,mask], y=ohaa, sample_weight=yy, reset_metrics=False)\n",
    "        return error\n",
    "    iter_under_policy(policy, iters=iters, games=games, rates=rates, epsilons=epsilons, proc=proc)\n",
    "                \n",
    "seed(42)\n",
    "pimodel1 = pimodel('pimodel1',18,32,9)  \n",
    "print(pimodel1.summary())\n",
    "\n",
    "if pimodel1.exists():\n",
    "    pimodel1.load()\n",
    "else:\n",
    "    train({1:pimodel1}, \n",
    "        discount = 0.99, \n",
    "        iters    = 300, \n",
    "        games    = 20, \n",
    "        rates    = {0:0.01, 200:0.001}, \n",
    "        epsilons = {0:0.1, 1000: 0.1})\n",
    "    pimodel1.save()\n",
    "mtime2str(pimodel1.path())\n",
    "\n",
    "agentpi = pimodel1\n",
    "rivalpi = lookaheadpi(randompi)\n",
    "testgames({1:agentpi, 2:rivalpi}, iters=TESTS)\n",
    "\n",
    "agentpi = maxpi(pimodel1)\n",
    "rivalpi = lookaheadpi(randompi)\n",
    "testgames({1:agentpi, 2:rivalpi}, iters=TESTS)\n",
    "\n",
    "enum_policies([\n",
    "    {1:agentpi, 2:rivalpi},\n",
    "    {1:agentpi, 2:qpi(dptable)},\n",
    "    {1:agentpi, 2:qpi(mctable)},\n",
    "    {1:agentpi, 2:qpi(tdtable)},\n",
    "    {1:agentpi, 2:qpi(mctsdata)},\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play(policy={1:maxpi(pimodel1)}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### POLICY-GRADIENTS PLAYER2 ###\n",
    "\n",
    "seed(1)\n",
    "pimodel2 = pimodel('pimodel2',18,32,9)  \n",
    "print(pimodel2.summary())\n",
    "\n",
    "allpi = choicepi(randompi,lookaheadpi(randompi),qpi(dptable),qpi(mctable),qpi(tdtable))\n",
    "\n",
    "if pimodel2.exists():\n",
    "    pimodel2.load()\n",
    "else:\n",
    "    train({2:pimodel2,1:allpi}, \n",
    "        discount = 0.99, \n",
    "        iters    = 100_000, \n",
    "        games    = 30, \n",
    "        rates    = {0:0.01, 30_000:0.001, 70_000:0.0001}, \n",
    "        epsilons = {0:0.0}\n",
    "    )\n",
    "    pimodel2.save()\n",
    "mtime2str(pimodel2.path())\n",
    "\n",
    "agentpi = pimodel2\n",
    "rivalpi = lookaheadpi(randompi)\n",
    "testgames({2:agentpi, 1:rivalpi}, iters=TESTS)\n",
    "\n",
    "agentpi = maxpi(pimodel2)\n",
    "rivalpi = lookaheadpi(randompi)\n",
    "testgames  ({2:agentpi, 1:rivalpi}, iters=TESTS)\n",
    "\n",
    "enum_policies([\n",
    "    {2:agentpi, 1:lookaheadpi(randompi)},\n",
    "    {2:agentpi, 1:qpi(dptable)},\n",
    "    {2:agentpi, 1:qpi(mctable)},\n",
    "    {2:agentpi, 1:qpi(tdtable)},\n",
    "    {2:agentpi, 1:qpi(mctsdata)},\n",
    "    {2:agentpi, 1:maxpi(pimodel1)},\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play(policy={2:maxpi(pimodel2)}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approximate Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### APPROXIMATE-Q-LEARNING ###\n",
    "\n",
    "class qmodel(Model):\n",
    "    def __init__(self, name, *nn, activation=None, buffer=100, batch=10):\n",
    "        def clone():\n",
    "            clone = qmodel(name,*nn,activation=activation)\n",
    "            clone.set_weights(self.get_weights())\n",
    "            return clone\n",
    "        x     = Input((nn[0],))\n",
    "        y     = x\n",
    "        for n in nn[1:-1]:\n",
    "            y = Dense(n, activation=activation)(y)\n",
    "        z     = Dense(nn[-1], activation='tanh')(y)\n",
    "        super(qmodel, self).__init__(inputs=x, outputs=z, name=name)\n",
    "        optimizer = Adam(lr=0.001, clipnorm=1.0)\n",
    "        self.compile(loss=mse, optimizer=optimizer) \n",
    "        self.__name__ = name        \n",
    "        self.buffer   = deque(maxlen=buffer*9) \n",
    "        self.batch    = batch*9\n",
    "        self.clone    = clone\n",
    "    def __call__(self, s, logits=False):\n",
    "        aa   = actions(s)\n",
    "        xx   = array([onehot(s)])\n",
    "        zz   = self.predict(xx)\n",
    "        qq   = ravel(zz)[aa]\n",
    "        if logits:\n",
    "            return aa,qq\n",
    "        pp   = softmaxprob(qq)\n",
    "        return aa,pp    \n",
    "    def push(self, xx, yy):\n",
    "        for x,y in zip(xx,yy):\n",
    "            self.buffer.append((x,y))\n",
    "    def pop(self):\n",
    "        n  = len(self.buffer)\n",
    "        ii = choice(arange(n), size=min(self.batch,n))\n",
    "        xx = array([self.buffer[i][0] for i in ii])\n",
    "        yy = array([self.buffer[i][1] for i in ii])\n",
    "        return xx,yy\n",
    "    def exchange(self, xx, yy):\n",
    "        self.push(xx, yy)\n",
    "        return self.pop()\n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "    def path(self):\n",
    "        return f'{DIR}/{self.__name__}.npy'\n",
    "    def save(self):\n",
    "        save(self.path(), self.get_weights())\n",
    "    def load(self):\n",
    "        weights = load(self.path())\n",
    "        self.set_weights(weights)\n",
    "    def exists(self):\n",
    "        return isfile(self.path())\n",
    "    \n",
    "def train(models, discount=0.9, iters=100, games=10, rates={0:0.1}, epsilons={0:0.1}):\n",
    "    agent,*_      = models.keys()\n",
    "    rival         = other(agent)\n",
    "    model         = models[agent]\n",
    "    policy        = {}    \n",
    "    policy[agent] = model\n",
    "    policy[rival] = models[rival] if rival in models else lookaheadpi(randompi)\n",
    "    def mcproc(ss,aa,pp,rr,rate=0.1):\n",
    "        K.set_value(model.optimizer.lr, rate)\n",
    "        yy = getoutcome(ss,aa,pp,rr,discount=discount)\n",
    "        xx = array([onehot(s) for s in ss])\n",
    "        zz = model.predict(xx)\n",
    "#         mask = array([actionmask(actions(s)) for s in ss])\n",
    "#         zz  *= mask\n",
    "        for z,a,y in zip(zz,aa,yy):\n",
    "            z[int(a)] = y\n",
    "        yy = zz\n",
    "        xx,yy = model.exchange(xx,yy)\n",
    "        error = model.train_on_batch(x=xx, y=yy, reset_metrics=False)\n",
    "        return error\n",
    "    iter_under_policy(policy, iters=iters, games=games, rates=rates, epsilons=epsilons, proc=mcproc)\n",
    "        \n",
    "seed(42)\n",
    "qmodelxo = qmodel('qmodelxo',18,36,9,activation='tanh',buffer=100,batch=20)  \n",
    "print(qmodelxo.summary())\n",
    "\n",
    "if qmodelxo.exists():\n",
    "    qmodelxo.load()\n",
    "else:\n",
    "    train({1:qmodelxo,2:qmodelxo}, \n",
    "          discount = 0.9, \n",
    "          iters    = 10_000, \n",
    "          games    = 10, \n",
    "          rates    = {0:0.1, 3_000:0.05, 5_000:0.01, 10_000:0.005, 15_000:0.001}, \n",
    "          epsilons = {0:0.1})\n",
    "    qmodelxo.save()\n",
    "mtime2str(qmodelxo.path())\n",
    "\n",
    "agentpi = maxpi(qmodelxo) \n",
    "rivalpi = lookaheadpi(randompi)\n",
    "\n",
    "testgames({1:agentpi, 2:rivalpi}, iters=TESTS)\n",
    "testgames({2:agentpi, 1:rivalpi}, iters=TESTS)\n",
    "\n",
    "enum_policies([\n",
    "    {1:agentpi, 2:rivalpi},\n",
    "    {1:agentpi, 2:qpi(dptable)},\n",
    "    {1:agentpi, 2:qpi(mctable)},\n",
    "    {1:agentpi, 2:qpi(tdtable)},\n",
    "    {1:agentpi, 2:qpi(mctsdata)},\n",
    "    {1:agentpi, 2:maxpi(pimodel2)},\n",
    "    {2:agentpi, 1:rivalpi},\n",
    "    {2:agentpi, 1:qpi(dptable)},\n",
    "    {2:agentpi, 1:qpi(mctable)},\n",
    "    {2:agentpi, 1:qpi(tdtable)},\n",
    "    {2:agentpi, 1:qpi(mctsdata)},\n",
    "    {2:agentpi, 1:maxpi(pimodel1)},\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play(policy={2:maxpi(qmodelxo)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scalable Approach: Monte Carlo Tree Search and Deep Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MCTS & DQN ###\n",
    "\n",
    "def makenode(value=0, parent=None, terminal=False):\n",
    "    return {'value'    : value,\n",
    "            'visits'   : 0,\n",
    "            'parent'   : parent,\n",
    "            'branches' : {} if not terminal else None\n",
    "           }\n",
    "\n",
    "def isterminal(node):\n",
    "    return node['branches'] is None\n",
    "\n",
    "def isexpanded(node):\n",
    "    return len(node['branches'] or {}) > 0\n",
    "\n",
    "def backpropagate(node, reward, rate=0.1):\n",
    "    while node is not None:\n",
    "        node['visits'] += 1\n",
    "        node['value']  += (reward - node['value']) / node['visits']\n",
    "#         node['value']  += (reward - node['value']) * rate\n",
    "        node            = node['parent']\n",
    "        reward          = -reward\n",
    "                \n",
    "def UCT(node, action, exploration): \n",
    "    n     = node['visits']\n",
    "    child = node['branches'][action]\n",
    "    nj    = child['visits']\n",
    "    xj    = child['value']\n",
    "    if exploration == 0:\n",
    "        return xj\n",
    "    if n == 0 or nj == 0:\n",
    "        return float('inf')\n",
    "    return xj + exploration*sqrt(2*log(n)/nj)\n",
    "\n",
    "def selectbranchpi(node, exploration=0):\n",
    "    assert(isexpanded(node))\n",
    "    actions  = list(node['branches'].keys())\n",
    "    values   = array([ UCT(node,a,exploration) for a in actions ])\n",
    "    probs    = argmaxprob(values)\n",
    "    return actions,probs\n",
    "\n",
    "def selectbranch(node, state, exploration):\n",
    "    actions,probs  = selectbranchpi(node, exploration)\n",
    "    action         = choice(actions, p=probs)\n",
    "    next_node      = node['branches'][action]\n",
    "    next_state,_   = game(state, action)\n",
    "    return next_node,next_state\n",
    "\n",
    "def expandbranch(model, node, state):\n",
    "    assert(not isterminal(node))\n",
    "    assert(not isexpanded(node))\n",
    "    branches = node['branches']\n",
    "    actor    = player(state)\n",
    "    aa,qq    = model(state, logits=True)\n",
    "    for a,q in zip(aa,qq):\n",
    "        _,winner    = game(state,a)\n",
    "        child       = makenode(value=q, parent=node, terminal=winner is not None)\n",
    "        branches[a] = child        \n",
    "        if isterminal(child):\n",
    "            reward = getreward(actor, winner) \n",
    "            backpropagate(child, reward) \n",
    "\n",
    "def selectnode(model, node, state, exploration):\n",
    "    while isexpanded(node):\n",
    "        node,state = selectbranch(node, state, exploration)\n",
    "    if not isterminal(node):\n",
    "        expandbranch(model, node, state)\n",
    "        node,state = selectbranch(node, state, exploration)\n",
    "    return node,state\n",
    "\n",
    "def simulatefrom(leaf, state, policy):\n",
    "    if isterminal(leaf):\n",
    "        return leaf['value']\n",
    "    actor        = player(state)\n",
    "    winner,_,_,_ = episode(policy, start=state)\n",
    "    reward       = getreward(actor, winner)\n",
    "    return reward \n",
    "        \n",
    "def search(model, node, state, policy, exploration=1):\n",
    "    leaf,state = selectnode(model, node, state, exploration)\n",
    "    reward     = simulatefrom(leaf, state, policy)\n",
    "    backpropagate(leaf, reward)\n",
    "    return reward\n",
    "\n",
    "last_action = None\n",
    "def selectaction(s,pi):\n",
    "    global last_action\n",
    "    aa,pp       = pi(s)\n",
    "    i           = choice(range(len(pp)), p=pp)\n",
    "    last_action = aa[i]\n",
    "    return aa[i],pp[i]\n",
    "\n",
    "def searchpi(model, \n",
    "             iters       = 1, \n",
    "             seconds     = 0, \n",
    "             exploration = 1, \n",
    "             policy      = {1:randompi,2:randompi}):\n",
    "    node = None\n",
    "    @rename(f'searchpi({model.name},{iters},{seconds}s,{policy[1].__name__}-vs-{policy[2].__name__})')\n",
    "    def searchpi(s):\n",
    "        nonlocal node\n",
    "        if node is None or all(s == 0):\n",
    "            node = makenode(value=0)\n",
    "        else:\n",
    "            node = node['branches'][last_action]\n",
    "        i,t  = iters,time()\n",
    "        while i > 0 or time() - t < seconds:\n",
    "            search(model, node, s, policy, exploration)\n",
    "            i -= 1\n",
    "        aa,pp = selectbranchpi(node, exploration=0)     \n",
    "        if exploration == 0:\n",
    "            node = None\n",
    "        return aa,pp\n",
    "    return searchpi\n",
    "\n",
    "def pushsample(model, state, action, reward):    \n",
    "    ss = rotate_and_flip(state)\n",
    "    aa = rotate_and_flip(action)\n",
    "    xx = array([onehot(s) for s in ss])\n",
    "    zz = model.predict(xx) \n",
    "    for z,a in zip(zz,aa):\n",
    "        z[int(a)] = reward\n",
    "    model.push(xx,zz)\n",
    "\n",
    "def train(model, \n",
    "          iters       = 1_000, \n",
    "          games       = 10, \n",
    "          searches    = 10, \n",
    "          exploration = 1, \n",
    "          discount    = 0.9, \n",
    "          clone_every = -1,\n",
    "          rates       = {0:0.1}):\n",
    "    agentpi  = searchpi(model, iters=searches, seconds=0, exploration=exploration)\n",
    "    print(f'searching under policy {agentpi.__name__}')\n",
    "    errors   = zeros(iters) \n",
    "    rewards  = [None, zeros(iters), zeros(iters)]\n",
    "    progbar  = Progbar(target=iters, stateful_metrics=['clone','rate'])\n",
    "    progress = PROGRESS(path=f'{DIR}/progress.log')\n",
    "    for i in range(iters):\n",
    "        if clone_every > 0: \n",
    "            if i % clone_every == 0:\n",
    "                clone   = model.clone()\n",
    "                agentpi = searchpi(clone, iters=searches, seconds=0, exploration=exploration)\n",
    "                clone   = i\n",
    "        else:\n",
    "            clone = i\n",
    "        if i in rates:\n",
    "            rate = rates[i]\n",
    "            K.set_value(model.optimizer.lr, rate)\n",
    "        ss,aa,pp,rr = samplegames(policy={1:agentpi,2:agentpi}, iters=games)\n",
    "        yy          = getoutcome(ss,aa,pp,rr,discount=discount)\n",
    "        for s,a,y in zip(ss,aa,yy):\n",
    "            pushsample(model,s,a,y)\n",
    "        for s,r in zip(ss,rr):\n",
    "            actor = player(s)\n",
    "            rewards[actor][i] += r/games         \n",
    "        xx,yy     = model.pop()\n",
    "        errors[i] = model.train_on_batch(x=xx, y=yy, reset_metrics=False)\n",
    "        values    = [\n",
    "            ('clone',  clone),\n",
    "            ('rate',   rate),\n",
    "            ('error',  errors[i]),\n",
    "            ('reward', rewards[1][i]),\n",
    "        ]        \n",
    "        progbar.update(i+1, values=values)\n",
    "        progress(f'{ipynb}, iter {i} of {iters}, {dict(values)}') \n",
    "        assert(rewards[1][i] == -rewards[2][i])\n",
    "    figure()\n",
    "    plot(rewards[1])\n",
    "    plot(rewards[2])\n",
    "    plot(errors,'r')\n",
    "    title('objective history')\n",
    "    ylabel('objective')\n",
    "    xlabel(f'games x{games}')\n",
    "    legend(['reward[X]', 'reward[O]', 'error'], loc='upper left')\n",
    "    savefig(f'{DIR}/{model.name}.png')\n",
    "    show()     \n",
    "\n",
    "seed(41)   \n",
    "          \n",
    "model = qmodel('mctsmodel',18,36,9,activation='tanh',buffer=100,batch=20)  \n",
    "print(model.summary())\n",
    "\n",
    "if model.exists():\n",
    "    model.load()\n",
    "else:\n",
    "    train(model, \n",
    "          iters       = 10_000, \n",
    "          games       = 10,\n",
    "          searches    = 10,\n",
    "          exploration = 10,\n",
    "          discount    = 0.9,s\n",
    "          clone_every = 1000,\n",
    "          rates       = {0:0.1, 1_000:0.05, 2_000:0.01, 5_000:0.005, 10_000:0.001, 20_000:0.0005, 25_000:0.0001})\n",
    "    model.save()\n",
    "mtime2str(model.path())\n",
    "\n",
    "agentpi = maxpi(model) \n",
    "\n",
    "testgames({1:agentpi, 2:lookaheadpi(randompi)}, iters=TESTS)\n",
    "testgames({2:agentpi, 1:lookaheadpi(randompi)}, iters=TESTS)\n",
    "\n",
    "enum_policies([\n",
    "    {1:agentpi, 2:randompi},\n",
    "    {1:agentpi, 2:lookaheadpi(randompi)},\n",
    "    {1:agentpi, 2:qpi(dptable)},\n",
    "    {1:agentpi, 2:qpi(mctable)},\n",
    "    {1:agentpi, 2:qpi(tdtable)},\n",
    "    {1:agentpi, 2:qpi(mctsdata)},\n",
    "    {1:agentpi, 2:maxpi(pimodel2)},\n",
    "    {2:agentpi, 1:randompi},\n",
    "    {2:agentpi, 1:lookaheadpi(randompi)},\n",
    "    {2:agentpi, 1:qpi(dptable)},\n",
    "    {2:agentpi, 1:qpi(mctable)},\n",
    "    {2:agentpi, 1:qpi(tdtable)},\n",
    "    {2:agentpi, 1:qpi(mctsdata)},\n",
    "    {2:agentpi, 1:maxpi(pimodel1)},\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agentpi = searchpi(model, iters=100, seconds=1, exploration=0, policy={1:randompi,2:randompi})\n",
    "agentpi = maxpi(model)\n",
    "play(policy={2:agentpi})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(model.buffer))\n",
    "\n",
    "xx = [ x for x,z in model.buffer ]\n",
    "xx.sort(key=count_nonzero)\n",
    "\n",
    "for i,x in enumerate(xx):\n",
    "    print(i, x, count_nonzero(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
